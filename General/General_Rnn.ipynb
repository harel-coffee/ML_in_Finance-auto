{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "General_Rnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l9o-8aJZgk9I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "1a57e9fc-7dc2-4cd4-cbec-a3e4d73a0e91"
      },
      "source": [
        ">>> import nltk\n",
        ">>> nltk.download('stopwords')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a5HnPotefAq7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2344efe1-b800-45b7-91be-e3b29c1ba8d0"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torchtext import data\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import spacy\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"GPU Model   \",torch.cuda.get_device_name(0))\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU Model    Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VCJKt_MyfAq-",
        "colab": {}
      },
      "source": [
        "def clean_data(X):\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "    X = X.str.lower()\n",
        "    X = X.str.replace(\"[/(){}\\[\\]\\|@,;]\", \" \")\n",
        "    X = X.str.replace(\"[^0-9a-z #+_]\", \" \")\n",
        "    X = X.str.replace(r'\\d+','')\n",
        "    X = X.apply(lambda x: ' '.join([w for w in x.split() if (len(w)>2 and w not in STOPWORDS) ] ))\n",
        "    X = X.apply(lambda x: x.split()) \n",
        "    return X\n",
        "\n",
        "def target_arrange(y):\n",
        "    \n",
        "    for i in range(len(y)):\n",
        "        if y.values[i]==\"Negative\":\n",
        "            y.values[i]=0.0\n",
        "        elif y[i]==\"Positive\":\n",
        "            y.values[i]=1.0\n",
        "        else:\n",
        "            y.values[i]=2.0\n",
        "            \n",
        "    y=y.to_numpy()  \n",
        "    y=y.reshape(y.shape[0],1)\n",
        "    y= pd.DataFrame(data=y)\n",
        "    y=np.ravel(y)\n",
        "    y=y.astype('float')\n",
        "    return y"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "39r_65tmfArB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "b681fc7b-2b5b-4866-e60e-a5595353d8a7"
      },
      "source": [
        "with open(\"All_Tickers.json\",\"r\") as fp:\n",
        "#with open(\"General_Market.json\",encoding='utf8') as fp:\n",
        "    json_d = json.load(fp)\n",
        " \n",
        "ticks_d = json_d['data']\n",
        "df = pd.DataFrame(ticks_d)\n",
        "\n",
        "X= pd.DataFrame(columns=['Date', 'Article','Target'])\n",
        "X['Date']=pd.to_datetime(df['date'])\n",
        "X['Article']=df['title']+\" \"+df['text']\n",
        "X['Target']=df['sentiment']\n",
        "\n",
        "X=X.sort_values(\"Date\")\n",
        "\n",
        "print(\"Number of Examples : \",len(X),\"\\n\")\n",
        "X.drop_duplicates(inplace=True)\n",
        "X.index = range(len(X))\n",
        "print(\"Number of Examples after removing duplicates: \",len(X),\"\\n\")\n",
        "\n",
        "print('Number of words before cleaning : ',X['Article'].apply(lambda x: len(str(x).split(' '))).sum())\n",
        "X['Article']=clean_data(X['Article'])\n",
        "print('Number of words after cleaning : ',X['Article'].apply(lambda x: len(str(x).split(' '))).sum())\n",
        "print(\"\\n******************\\n\")\n",
        "\n",
        "X['Target']=target_arrange(X['Target'])\n",
        "X=X.drop('Date',1)\n",
        "L=[]\n",
        "L.append((X['Target']==0.0).sum())\n",
        "L.append((X['Target']==1.0).sum())\n",
        "L.append((X['Target']==2.0).sum())\n",
        "\n",
        "print(\"Negative Examples : \",(X['Target']==0.0).sum())\n",
        "print(\"Positive Examples : \",(X['Target']==1.0).sum())\n",
        "print(\"Neutral Examples : \",(X['Target']==2.0).sum())\n",
        "\n",
        "maximum=max(L)\n",
        "\n",
        "Weights=[]\n",
        "\n",
        "for i in L:\n",
        "  Weights.append(maximum/i)\n",
        "  \n",
        "class_weights = torch.FloatTensor(Weights).to(device)\n",
        "print(\"\\n Weights = \",class_weights)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Examples :  28130 \n",
            "\n",
            "Number of Examples after removing duplicates:  27937 \n",
            "\n",
            "Number of words before cleaning :  1067277\n",
            "Number of words after cleaning :  718391\n",
            "\n",
            "******************\n",
            "\n",
            "Negative Examples :  3143\n",
            "Positive Examples :  9901\n",
            "Neutral Examples :  14893\n",
            "\n",
            " Weights =  tensor([4.7385, 1.5042, 1.0000], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F2Lyyf7vfArF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "3a436d9a-83cb-473f-f2ca-91b432148cc9"
      },
      "source": [
        "X['Article']= X['Article'].apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
        "for i in range(len(X['Article'])): #φέρνω τα tokens ξανά μαζί διαχωριζόμενα με κενά\n",
        "    X['Article'][i] = ' '.join(X['Article'][i])\n",
        "print(X['Article'])\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0        vianet group inc announc unaudit second quarte...\n",
            "1        krato present canaccord virtual growth confer ...\n",
            "2        rewalk robot report second quarter financi res...\n",
            "3        pyxi tanker announc date releas second quarter...\n",
            "4        bionano genom report second quarter financi re...\n",
            "                               ...                        \n",
            "27932    facebook block new polit ad may fall short sti...\n",
            "27933    amazon growth problem buy good good enough amz...\n",
            "27934    pyrogenesi sign contract navi two ship build p...\n",
            "27935    guardant health high price lot potenti guardan...\n",
            "27936    oil futur post first weekli fall week oil futu...\n",
            "Name: Article, Length: 27937, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v_A1300jfArI",
        "colab": {}
      },
      "source": [
        "X.to_csv (r'General_rnn.csv', index = False, header=True)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HmOB68OSfArK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "680f9b5a-226c-4b83-ae6c-6ced506f752c"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "batch=1024\n",
        "#TEXT = data.Field(tokenize='spacy',batch_first=True,include_lengths=True)\n",
        "TEXT = data.Field(batch_first=True,include_lengths=True)\n",
        "LABEL = data.LabelField(dtype = torch.long,batch_first=True)\n",
        "\n",
        "fields = [('text',TEXT),('label', LABEL)]\n",
        "Train_Data=data.TabularDataset(path = 'General_rnn.csv',format = 'csv',fields = fields,skip_header = True)\n",
        "\n",
        "X_train, X_test = Train_Data.split(split_ratio=0.75, random_state = random.seed(1234))\n",
        "X_train, X_val = X_train.split(split_ratio=0.8, random_state = random.seed(1234))\n",
        "\n",
        "TEXT.build_vocab(X_train, min_freq=2)  \n",
        "LABEL.build_vocab(X_train)\n",
        "\n",
        "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
        "\n",
        "#No. of unique tokens in label\n",
        "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
        "\n",
        "#Commonly used words\n",
        "print(TEXT.vocab.freqs.most_common(10))  \n",
        "train_it, val_it, test_it = data.BucketIterator.splits((X_train, X_val, X_test),sort_key = lambda x: len(x.text),\n",
        "    sort_within_batch=True,batch_size = batch,device = device)  \n",
        "\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of TEXT vocabulary: 14528\n",
            "Size of LABEL vocabulary: 3\n",
            "[('stock', 6745), ('earn', 5809), ('inc', 5119), ('compani', 4671), ('announc', 4504), ('result', 4210), ('report', 3204), ('busi', 2862), ('new', 2743), ('investor', 2680)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hIvY8I8tfArM",
        "colab": {}
      },
      "source": [
        "def find_accuracy(preds, y):\n",
        "   \n",
        "    temp = torch.log_softmax(preds, dim = 1)\n",
        "    _, y_pred = torch.max(temp, dim = 1) \n",
        "    valid = (y_pred == y).float() \n",
        "    accur = valid.sum() / len(valid)\n",
        "    return accur\n",
        "\n",
        "def Loss_Optimizer (model,valueLR):\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=valueLR)\n",
        "    return optimizer,criterion"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lNSC2cF4fArO",
        "colab": {}
      },
      "source": [
        "def train(model,data,lr,optimizer,criterion):\n",
        "        \n",
        "    #optimizer,criterion=Loss_Optimizer (model,lr)\n",
        "    #criterion = criterion.to(device)\n",
        "    model.train()\n",
        "    sumloss=0.0\n",
        "    sumacc=0.0\n",
        "    for i in data:\n",
        "        text, text_lengths = i.text   \n",
        "        optimizer.zero_grad()\n",
        "        pred = model(text, text_lengths).squeeze(0)       \n",
        "        loss = criterion(pred, i.label)\n",
        "        acc = find_accuracy(pred, i.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        sumloss += loss.item()\n",
        "        sumacc += acc.item()\n",
        "          \n",
        "    return sumloss / len(data), sumacc / len(data)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bqLTVN39fArR",
        "colab": {}
      },
      "source": [
        "def test(model, data, criterion):\n",
        "    \n",
        "    sumloss = 0\n",
        "    sumacc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i in data:\n",
        "            text, text_lengths = i.text\n",
        "            \n",
        "            pred = model(text, text_lengths).squeeze(0)\n",
        "           \n",
        "            loss = criterion(pred, i.label)\n",
        "            \n",
        "            acc = find_accuracy(pred, i.label)\n",
        "\n",
        "            sumloss += loss.item()\n",
        "            sumacc += acc.item()\n",
        "        \n",
        "    return sumloss / len(data), sumacc / len(data)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GnTkljWVfArT",
        "colab": {}
      },
      "source": [
        "def process_test(model,numEpochs,data_tr,data_val,data_test,lr,loss_file,acc_file,best_model):\n",
        "\n",
        "    optimizer,criterion=Loss_Optimizer (model,lr)\n",
        "    criterion = criterion.to(device)\n",
        "    start_time = time.time()\n",
        "    fileout=open(loss_file,\"w\")\n",
        "    fileout2=open(acc_file,\"w\")\n",
        "    \n",
        "    for i in range(numEpochs):\n",
        " \n",
        "        train_loss, train_acc = train(model, data_tr, lr,optimizer,criterion)\n",
        "        valid_loss, valid_acc = test(model, data_val, criterion)\n",
        "        fileout.write(str(valid_loss)+\"\\n\")\n",
        "        fileout2.write(str(valid_acc)+\"\\n\")\n",
        "        if i==0:\n",
        "            max=valid_acc\n",
        "            torch.save(model.state_dict(), best_model)\n",
        "        elif valid_acc > max:\n",
        "            max = valid_acc\n",
        "            torch.save(model.state_dict(), best_model)\n",
        "\n",
        "        if (i+1)%5==0:\n",
        "          print(\"Epoch : \",i+1,\" Train Loss : \",train_loss,\"  Train Acc : \",train_acc,\"  Valid Loss : \",valid_loss, \" Val Acc : \",valid_acc)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    timeHelp=(end_time-start_time)/60.0\n",
        "    print(\"\\nTime needed for Training : \",timeHelp)\n",
        "\n",
        "    fileout.close()\n",
        "    fileout2.close()  \n",
        "    \n",
        "   \n",
        "    model.load_state_dict(torch.load(best_model))\n",
        "    \n",
        "    test_loss, test_acc = test(model, data_test, criterion)\n",
        "    print(\"\\nLoss in Testset : \",test_loss,\"  Accuracy in Testset : \",test_acc,\"\\n\")"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OTVfTSDHfArW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20181e00-3849-4787-b406-56963bf32a5c"
      },
      "source": [
        "print(\"GPU Model   \",torch.cuda.get_device_name(0))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU Model    Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iN7ov3gHfArY",
        "colab": {}
      },
      "source": [
        "class myLSTM(nn.Module):\n",
        "    def __init__(self, voc, embed_d, hid_d, out_d,dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(voc, embed_d)\n",
        "        self.rnn = nn.LSTM(embed_d,hid_d,batch_first=True,num_layers=2,bidirectional=True,dropout=dropout)\n",
        "        self.fc = nn.Linear(hid_d * 2, out_d)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, input_text, text_lengths):\n",
        "                \n",
        "        embedded = self.dropout(self.embedding(input_text))\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n",
        "        rnn_out,(rnn_hid,rnn_cell) = self.rnn(packed_embedded)\n",
        "        hidden = self.dropout(torch.cat((rnn_hid[-2,:,:], rnn_hid[-1,:,:]), dim=1))\n",
        "        return self.fc(hidden)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4OCyMqTxfArb",
        "colab": {}
      },
      "source": [
        "class myGRU(nn.Module):\n",
        "    def __init__(self, voc, embed_d, hid_d, out_d,dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(voc, embed_d)\n",
        "        self.rnn = nn.GRU(embed_d,hid_d,num_layers=2,bidirectional=True,dropout=dropout)\n",
        "        self.fc = nn.Linear(hid_d * 2, out_d)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, input_text,text_lengths):\n",
        "                \n",
        "        embedded = self.dropout(self.embedding(input_text))\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n",
        "        rnn_out,hidden = self.rnn(packed_embedded)\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "        return self.fc(hidden)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tMbHHUnifArd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "a4db3d72-7744-4032-8930-f02ea61ff818"
      },
      "source": [
        "myInput = len(TEXT.vocab)\n",
        "myEmbed = 300\n",
        "myHid = 256\n",
        "myOut = 3\n",
        "dropout = 0.4\n",
        "\n",
        "new_model2 = myLSTM(myInput, myEmbed, myHid, myOut,dropout)\n",
        "new_model2.to(torch.device(device))\n",
        "new_model3 = myGRU(myInput, myEmbed, myHid, myOut,dropout)\n",
        "new_model3.to(torch.device(device))\n",
        "\n",
        "print(new_model2)\n",
        "print(new_model3)\n",
        "#torch.save(new_model2.state_dict(), 'model_LSTM_1.pt')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "myLSTM(\n",
            "  (embedding): Embedding(14528, 300)\n",
            "  (rnn): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
            "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
            "  (dropout): Dropout(p=0.4, inplace=False)\n",
            ")\n",
            "myGRU(\n",
            "  (embedding): Embedding(14528, 300)\n",
            "  (rnn): GRU(300, 256, num_layers=2, dropout=0.4, bidirectional=True)\n",
            "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
            "  (dropout): Dropout(p=0.4, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xEMSmzBOfArf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "312760f1-5191-43a2-f441-9d1070b32d35"
      },
      "source": [
        "process_test(new_model2,30,train_it,val_it,test_it,0.001,\"lstm1_loss.txt\",\"lstm1_acc.txt\",\"best_LSTM_1_model.pt\")"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch :  5  Train Loss :  0.3259941427694524   Train Acc :  0.7471277538467856   Valid Loss :  0.30067711481824516  Val Acc :  0.7822265625\n",
            "Epoch :  10  Train Loss :  0.1789632415097645   Train Acc :  0.8730468714938444   Valid Loss :  0.3545713071856881  Val Acc :  0.8177734375\n",
            "Epoch :  15  Train Loss :  0.10191365624504055   Train Acc :  0.9342256398761973   Valid Loss :  0.4914697307627648  Val Acc :  0.83984375\n",
            "Epoch :  20  Train Loss :  0.053707805460469576   Train Acc :  0.9649011913467856   Valid Loss :  0.6354317214019829  Val Acc :  0.851171875\n",
            "Epoch :  25  Train Loss :  0.0408472887801883   Train Acc :  0.9740923678173738   Valid Loss :  0.7163185386343685  Val Acc :  0.854296875\n",
            "Epoch :  30  Train Loss :  0.03002985843169667   Train Acc :  0.9800666325232562   Valid Loss :  0.7014689822448418  Val Acc :  0.848828125\n",
            "\n",
            "Time needed for Training :  1.7363901734352112\n",
            "\n",
            "Loss in Testset :  0.8401467885289874   Accuracy in Testset :  0.8282538822719029 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ufEdn9rhfAri",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "3fe4223d-00a3-4c25-d2c0-e7cabb59599a"
      },
      "source": [
        "process_test(new_model3,30,train_it,val_it,test_it,0.001,\"gru1_loss.txt\",\"gru1_acc.txt\",\"best_gru_1_model.pt\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch :  5  Train Loss :  0.330435328240342   Train Acc :  0.7508042244350209   Valid Loss :  0.2921428566798568  Val Acc :  0.801953125\n",
            "Epoch :  10  Train Loss :  0.20542483986355364   Train Acc :  0.8602941141409033   Valid Loss :  0.3412595229223371  Val Acc :  0.833203125\n",
            "Epoch :  15  Train Loss :  0.13450773693415718   Train Acc :  0.9131433788467856   Valid Loss :  0.4300518475472927  Val Acc :  0.8350041151046753\n",
            "Epoch :  20  Train Loss :  0.08223554554997999   Train Acc :  0.9439338200232562   Valid Loss :  0.510126031190157  Val Acc :  0.8504338026046753\n",
            "Epoch :  25  Train Loss :  0.058786326736855486   Train Acc :  0.9611672759056091   Valid Loss :  0.6737908825278283  Val Acc :  0.8518009901046752\n",
            "Epoch :  30  Train Loss :  0.03969232612357968   Train Acc :  0.974781706052668   Valid Loss :  0.7627447455619404  Val Acc :  0.84921875\n",
            "\n",
            "Time needed for Training :  1.4233663082122803\n",
            "\n",
            "Loss in Testset :  0.6613414511084557   Accuracy in Testset :  0.8308434401239667 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}