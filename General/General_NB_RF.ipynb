{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection,metrics,naive_bayes,preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "stemmer = PorterStemmer()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(X):\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    X=X.str.lower()\n",
    "    X=X.str.replace(\"[/(){}\\[\\]\\|@,;]\", \" \")\n",
    "    X=X.str.replace(\"[^0-9a-z #+_]\", \" \")\n",
    "    X = X.str.replace(r'\\d+','')\n",
    "    X = X.apply(lambda x: ' '.join([w for w in x.split() if (len(w)>2 and w not in STOPWORDS) ] ))\n",
    "    X = X.apply(lambda x: x.split()) \n",
    "    return X\n",
    "\n",
    "def target_arrange(y):\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        if y.values[i]==\"Negative\":\n",
    "            y.values[i]=0.0\n",
    "        elif y[i]==\"Positive\":\n",
    "            y.values[i]=1.0\n",
    "        else:\n",
    "            y.values[i]=2.0\n",
    "            \n",
    "    y=y.to_numpy()  \n",
    "    y=y.reshape(y.shape[0],1)\n",
    "    y= pd.DataFrame(data=y)\n",
    "    y=np.ravel(y)\n",
    "    y=y.astype('float')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Examples :  28130 \n",
      "\n",
      "Number of Examples after removing duplicates:  27937 \n",
      "\n",
      "Number of words before cleaning :  1067277\n",
      "Number of words after cleaning :  718391\n",
      "           Date  Article\n",
      "Target                  \n",
      "Negative   3143     3143\n",
      "Neutral   14893    14893\n",
      "Positive   9901     9901 \n",
      "\n",
      "0       2020-08-07 08:00:00-04:00\n",
      "1       2020-08-07 08:00:00-04:00\n",
      "2       2020-08-07 08:00:00-04:00\n",
      "3       2020-08-07 08:00:00-04:00\n",
      "4       2020-08-07 08:00:00-04:00\n",
      "                   ...           \n",
      "27932   2020-09-04 14:33:55-04:00\n",
      "27933   2020-09-04 14:34:56-04:00\n",
      "27934   2020-09-04 14:35:00-04:00\n",
      "27935   2020-09-04 14:35:12-04:00\n",
      "27936   2020-09-04 14:39:51-04:00\n",
      "Name: Date, Length: 27937, dtype: datetime64[ns, pytz.FixedOffset(-240)]\n"
     ]
    }
   ],
   "source": [
    "with open(\"All_Tickers.json\",\"r\") as fp:\n",
    "#with open(\"General_Market.json\",encoding='utf8') as fp:\n",
    "    json_d = json.load(fp)\n",
    " \n",
    "ticks_d = json_d['data']\n",
    "df = pd.DataFrame(ticks_d)\n",
    "\n",
    "X= pd.DataFrame(columns=['Date', 'Article','Target'])\n",
    "X['Date']=pd.to_datetime(df['date'])\n",
    "X['Article']=df['title']+\" \"+df['text']\n",
    "X['Target']=df['sentiment']\n",
    "\n",
    "X=X.sort_values(\"Date\")\n",
    "\n",
    "print(\"Number of Examples : \",len(X),\"\\n\")\n",
    "X.drop_duplicates(inplace=True)\n",
    "X.index = range(len(X))\n",
    "print(\"Number of Examples after removing duplicates: \",len(X),\"\\n\")\n",
    "\n",
    "\n",
    "X.to_csv (r'General.csv', index = False, header=True)\n",
    "\n",
    "print('Number of words before cleaning : ',X['Article'].apply(lambda x: len(str(x).split(' '))).sum())\n",
    "X['Article']=clean_data(X['Article'])\n",
    "print('Number of words after cleaning : ',X['Article'].apply(lambda x: len(str(x).split(' '))).sum())\n",
    "\n",
    "print(X.groupby(['Target']).count(),\"\\n\")\n",
    "print(X['Date'])\n",
    "y=target_arrange(X['Target'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        vianet group inc announc unaudit second quarte...\n",
      "1        krato present canaccord virtual growth confer ...\n",
      "2        rewalk robot report second quarter financi res...\n",
      "3        pyxi tanker announc date releas second quarter...\n",
      "4        bionano genom report second quarter financi re...\n",
      "                               ...                        \n",
      "27932    facebook block new polit ad may fall short sti...\n",
      "27933    amazon growth problem buy good good enough amz...\n",
      "27934    pyrogenesi sign contract navi two ship build p...\n",
      "27935    guardant health high price lot potenti guardan...\n",
      "27936    oil futur post first weekli fall week oil futu...\n",
      "Name: Article, Length: 27937, dtype: object\n"
     ]
    }
   ],
   "source": [
    "X['Article']= X['Article'].apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "for i in range(len(X['Article'])): #φέρνω τα tokens ξανά μαζί διαχωριζόμενα με κενά\n",
    "    X['Article'][i] = ' '.join(X['Article'][i])\n",
    "\n",
    "print(X['Article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9)\n",
    "Xv = tfidf_vectorizer.fit_transform(X['Article'])\n",
    "Xv = pd.DataFrame(Xv.todense())\n",
    "X_train,X_test,y_train,y_test = train_test_split(Xv,y, test_size=0.25,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Recall metric: 0.6775030111877958\n",
      " F1 metric: 0.6862463876004958\n",
      " Precision metric: 0.7149336588885277\n",
      " Accuracy metric: 0.7570508231925555\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1 # This is the smoothing parameter for Laplace/Lidstone smoothing\n",
    "model = naive_bayes.MultinomialNB(alpha=alpha)\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "\n",
    "recall = metrics.recall_score(y_test,y_predicted,average='macro')\n",
    "precision = metrics.precision_score(y_test,y_predicted,average='macro')\n",
    "f1 = metrics.f1_score(y_test,y_predicted,average='macro')\n",
    "Accur=metrics.accuracy_score(y_test,y_predicted)\n",
    "\n",
    "print(' Recall metric:',recall)\n",
    "print(' F1 metric:',f1)\n",
    "print(' Precision metric:',precision)\n",
    "print(' Accuracy metric:',Accur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7994405601304491\n"
     ]
    }
   ],
   "source": [
    "skfold = model_selection.StratifiedKFold(n_splits=5)\n",
    "model2 = RandomForestClassifier(n_estimators=100, max_features=\"auto\", n_jobs=-1)\n",
    "results = model_selection.cross_val_score(model2, Xv,y, cv=skfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Recall metric: 0.6988502062134616\n",
      " F1 metric: 0.7203128953596843\n",
      " Precision metric: 0.8137274988487069\n",
      " Accuracy metric: 0.8044380816034359\n"
     ]
    }
   ],
   "source": [
    "model2 = RandomForestClassifier(n_estimators=100, max_features=\"auto\", n_jobs=-1)\n",
    "model2.fit(X_train,y_train)\n",
    "y_predicted = model2.predict(X_test)\n",
    "\n",
    "\n",
    "recall = metrics.recall_score(y_test,y_predicted,average='macro')\n",
    "precision = metrics.precision_score(y_test,y_predicted,average='macro')\n",
    "f1 = metrics.f1_score(y_test,y_predicted,average='macro')\n",
    "Accur=metrics.accuracy_score(y_test,y_predicted)\n",
    "\n",
    "print(' Recall metric:',recall)\n",
    "print(' F1 metric:',f1)\n",
    "print(' Precision metric:',precision)\n",
    "print(' Accuracy metric:',Accur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
