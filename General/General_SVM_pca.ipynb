{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection,metrics,naive_bayes,preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "stemmer = PorterStemmer()\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "mypca = PCA(0.95)\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(X):\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    X=X.str.lower()\n",
    "    X=X.str.replace(\"[/(){}\\[\\]\\|@,;]\", \" \")\n",
    "    X=X.str.replace(\"[^0-9a-z #+_]\", \" \")\n",
    "    X = X.str.replace(r'\\d+','')\n",
    "    X = X.apply(lambda x: ' '.join([w for w in str(x).split() if (len(w)>2 and w not in STOPWORDS) ] ))\n",
    "    X = X.apply(lambda x: x.split()) \n",
    "    return X\n",
    "\n",
    "def target_arrange(y):\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        if y.values[i]==\"Negative\":\n",
    "            y.values[i]=0.0\n",
    "        elif y[i]==\"Positive\":\n",
    "            y.values[i]=1.0\n",
    "        else:\n",
    "            y.values[i]=2.0\n",
    "            \n",
    "    y=y.to_numpy()  \n",
    "    y=y.reshape(y.shape[0],1)\n",
    "    y= pd.DataFrame(data=y)\n",
    "    y=np.ravel(y)\n",
    "    y=y.astype('float')\n",
    "    return y\n",
    "\n",
    "def select_n_components(ratio, goal):\n",
    "        # Set initial variance explained so far\n",
    "        s=0.0 \n",
    "        # Set initial number of features\n",
    "        num_components = 0\n",
    "\n",
    "        for i in ratio:\n",
    "            s += i\n",
    "            num_components += 1\n",
    "            if s >= goal:\n",
    "                break\n",
    "\n",
    "        # Return the number of components\n",
    "        return num_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Examples :  28130 \n",
      "\n",
      "Number of Examples after removing duplicates:  27937 \n",
      "\n",
      "Number of words before cleaning :  1067277\n",
      "Number of words after cleaning :  718391\n",
      "           Date  Article\n",
      "Target                  \n",
      "Negative   3143     3143\n",
      "Neutral   14893    14893\n",
      "Positive   9901     9901\n",
      "0       2020-08-07 08:00:00-04:00\n",
      "1       2020-08-07 08:00:00-04:00\n",
      "2       2020-08-07 08:00:00-04:00\n",
      "3       2020-08-07 08:00:00-04:00\n",
      "4       2020-08-07 08:00:00-04:00\n",
      "                   ...           \n",
      "27932   2020-09-04 14:33:55-04:00\n",
      "27933   2020-09-04 14:34:56-04:00\n",
      "27934   2020-09-04 14:35:00-04:00\n",
      "27935   2020-09-04 14:35:12-04:00\n",
      "27936   2020-09-04 14:39:51-04:00\n",
      "Name: Date, Length: 27937, dtype: datetime64[ns, pytz.FixedOffset(-240)]\n"
     ]
    }
   ],
   "source": [
    "with open(\"All_Tickers.json\",\"r\") as fp:\n",
    "#with open(\"General_Market.json\",encoding='utf8') as fp:\n",
    "    json_d = json.load(fp)\n",
    " \n",
    "ticks_d = json_d['data']\n",
    "df = pd.DataFrame(ticks_d)\n",
    "\n",
    "X= pd.DataFrame(columns=['Date', 'Article','Target'])\n",
    "X['Date']=pd.to_datetime(df['date'])\n",
    "X['Article']=df['title']+\" \"+df['text']\n",
    "X['Target']=df['sentiment']\n",
    "\n",
    "X=X.sort_values(\"Date\")\n",
    "print(\"Number of Examples : \",len(X),\"\\n\")\n",
    "X.drop_duplicates(inplace=True)\n",
    "X.index = range(len(X))\n",
    "print(\"Number of Examples after removing duplicates: \",len(X),\"\\n\")\n",
    "X.to_csv (r'General_pca.csv', index = False, header=True)\n",
    "\n",
    "print('Number of words before cleaning : ',X['Article'].apply(lambda x: len(str(x).split(' '))).sum())\n",
    "X['Article']=clean_data(X['Article'])\n",
    "print('Number of words after cleaning : ',X['Article'].apply(lambda x: len(str(x).split(' '))).sum())\n",
    "\n",
    "print(X.groupby(['Target']).count())\n",
    "print(X['Date'])\n",
    "y=target_arrange(X['Target'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        vianet group inc announc unaudit second quarte...\n",
      "1        krato present canaccord virtual growth confer ...\n",
      "2        rewalk robot report second quarter financi res...\n",
      "3        pyxi tanker announc date releas second quarter...\n",
      "4        bionano genom report second quarter financi re...\n",
      "                               ...                        \n",
      "27932    facebook block new polit ad may fall short sti...\n",
      "27933    amazon growth problem buy good good enough amz...\n",
      "27934    pyrogenesi sign contract navi two ship build p...\n",
      "27935    guardant health high price lot potenti guardan...\n",
      "27936    oil futur post first weekli fall week oil futu...\n",
      "Name: Article, Length: 27937, dtype: object\n"
     ]
    }
   ],
   "source": [
    "X['Article']= X['Article'].apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "for i in range(len(X['Article'])): #φέρνω τα tokens ξανά μαζί διαχωριζόμενα με κενά\n",
    "    X['Article'][i] = ' '.join(X['Article'][i])\n",
    "\n",
    "print(X['Article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9)\n",
    "Xv = tfidf_vectorizer.fit_transform(X['Article'])\n",
    "Xv = pd.DataFrame(Xv.todense())\n",
    "X_train,X_test,y_train,y_test = train_test_split(Xv,y, test_size=0.25,stratify=y)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = mypca.fit_transform(X_train)\n",
    "X_test = mypca.transform(X_test)\n",
    "\n",
    "X_lda = lda.fit(X_train, y_train)\n",
    "\n",
    "X_train=lda.transform(X_train)\n",
    "X_test=lda.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'gamma': 1e-05, 'kernel': 'rbf'}\n",
      " Recall metric: 0.7574434759557872\n",
      " F1 metric: 0.7499986320958186\n",
      " Precision metric: 0.7444729742826985\n",
      " Accuracy metric: 0.7909806728704366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "#mykpca=KernelPCA(kernel=\"rbf\")\n",
    "#X_train_rbf1=mykpca.fit_transform(X_train)\n",
    "#X_test_rbf1=mykpca.transform(X_test)\n",
    "\n",
    "tuned_parameters = [\n",
    "  {'C': [0.01,0.03, 0.1, 1, 10, 100 ], 'gamma': [5, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001], 'kernel': ['rbf']},\n",
    "  \n",
    " ]\n",
    "\n",
    "grid = GridSearchCV(SVC(class_weight='balanced', decision_function_shape='ovo'), tuned_parameters, n_jobs=-1, refit = True, cv=5) \n",
    "grid.fit(X_train, y_train) \n",
    "print(grid.best_params_) \n",
    "grid_predictions = grid.predict(X_test) \n",
    "recall = metrics.recall_score(y_test,grid_predictions,average='macro')\n",
    "precision = metrics.precision_score(y_test,grid_predictions,average='macro')\n",
    "f1 = metrics.f1_score(y_test,grid_predictions,average='macro')\n",
    "Accur=metrics.accuracy_score(y_test,grid_predictions)\n",
    "\n",
    "print(' Recall metric:',recall)\n",
    "print(' F1 metric:',f1)\n",
    "print(' Precision metric:',precision)\n",
    "print(' Accuracy metric:',Accur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
