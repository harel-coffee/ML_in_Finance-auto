{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Tech_Rnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l9o-8aJZgk9I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "cf0dce1b-d8a5-4ee6-d9c5-31659aa6af51"
      },
      "source": [
        ">>> import nltk\n",
        ">>> nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a5HnPotefAq7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "671914f5-0ddd-4e9e-f959-c4e96c399202"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torchtext import data\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import spacy\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"GPU Model   \",torch.cuda.get_device_name(0))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU Model    Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VCJKt_MyfAq-",
        "colab": {}
      },
      "source": [
        "def clean_data(X):\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "    X=X.str.lower()\n",
        "    X=X.str.replace(\"[/(){}\\[\\]\\|@,;]\", \" \")\n",
        "    X=X.str.replace(\"[^0-9a-z #+_]\", \" \")\n",
        "    X = X.str.replace(r'\\d+','')\n",
        "    X = X.apply(lambda x: ' '.join([w for w in str(x).split() if (len(w)>2 and w not in STOPWORDS) ] ))\n",
        "    X = X.apply(lambda x: x.split()) \n",
        "    return X\n",
        "\n",
        "def target_arrange(y):\n",
        "    \n",
        "    for i in range(len(y)):\n",
        "        if y.values[i]==\"Negative\":\n",
        "            y.values[i]=0.0\n",
        "        elif y[i]==\"Positive\":\n",
        "            y.values[i]=1.0\n",
        "        else:\n",
        "            y.values[i]=2.0\n",
        "            \n",
        "    y=y.to_numpy()  \n",
        "    y=y.reshape(y.shape[0],1)\n",
        "    y= pd.DataFrame(data=y)\n",
        "    y=np.ravel(y)\n",
        "    y=y.astype('float')\n",
        "    return y"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "39r_65tmfArB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "1b938953-3a31-4943-f22b-af342bd68d4f"
      },
      "source": [
        "with open(\"Tech_news.json\",\"r\") as fp:\n",
        "#with open(\"General_Market.json\",encoding='utf8') as fp:\n",
        "    json_d = json.load(fp)\n",
        " \n",
        "ticks_d = json_d['data']\n",
        "df = pd.DataFrame(ticks_d)\n",
        "\n",
        "X= pd.DataFrame(columns=['Date', 'Article','Target'])\n",
        "X['Date']=pd.to_datetime(df['date'])\n",
        "X['Article']=df['title']+\" \"+df['text']\n",
        "X['Target']=df['sentiment']\n",
        "\n",
        "X=X.sort_values(\"Date\")\n",
        "\n",
        "print(\"Number of Examples : \",len(X),\"\\n\")\n",
        "X.drop_duplicates(inplace=True)\n",
        "X.index = range(len(X))\n",
        "print(\"Number of Examples after removing duplicates: \",len(X),\"\\n\")\n",
        "\n",
        "print('Number of words before cleaning : ',X['Article'].apply(lambda x: len(str(x).split(' '))).sum())\n",
        "X['Article']=clean_data(X['Article'])\n",
        "print('Number of words after cleaning : ',X['Article'].apply(lambda x: len(str(x).split(' '))).sum())\n",
        "print(\"\\n******************\\n\")\n",
        "#print(X.groupby(['Target']).count())\n",
        "X['Target']=target_arrange(X['Target'])\n",
        "X=X.drop('Date',1)\n",
        "\n",
        "L=[]\n",
        "L.append((X['Target']==0.0).sum())\n",
        "L.append((X['Target']==1.0).sum())\n",
        "L.append((X['Target']==2.0).sum())\n",
        "\n",
        "print(\"Negative Examples : \",(X['Target']==0.0).sum())\n",
        "print(\"Positive Examples : \",(X['Target']==1.0).sum())\n",
        "print(\"Neutral Examples : \",(X['Target']==2.0).sum())\n",
        "\n",
        "maximum=max(L)\n",
        "\n",
        "Weights=[]\n",
        "\n",
        "for i in L:\n",
        "  Weights.append(maximum/i)\n",
        "  \n",
        "class_weights = torch.FloatTensor(Weights).to(device)\n",
        "print(\"\\n Weights = \",class_weights)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Examples :  16651 \n",
            "\n",
            "Number of Examples after removing duplicates:  16413 \n",
            "\n",
            "Number of words before cleaning :  619298\n",
            "Number of words after cleaning :  416411\n",
            "\n",
            "******************\n",
            "\n",
            "Negative Examples :  2040\n",
            "Positive Examples :  6195\n",
            "Neutral Examples :  8178\n",
            "\n",
            " Weights =  tensor([4.0088, 1.3201, 1.0000], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F2Lyyf7vfArF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "a8db900c-2dac-4938-aa3f-912b0fcf66d8"
      },
      "source": [
        "X['Article']= X['Article'].apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
        "for i in range(len(X['Article'])): #φέρνω τα tokens ξανά μαζί διαχωριζόμενα με κενά\n",
        "    X['Article'][i] = ' '.join(X['Article'][i])\n",
        "print(X['Article'])\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0        acuiti brand declar quarterli dividend atlanta...\n",
            "1        progress second quarter revenu exce guidanc in...\n",
            "2        mercuri system receiv contract award base new ...\n",
            "3         share factset soar today earn came better expect\n",
            "4        stifel say inseego leader inseego corp nasdaq ...\n",
            "                               ...                        \n",
            "16408    adob stock fall today investor take profit mad...\n",
            "16409    dow jone drop point appl salesforc stock head ...\n",
            "16410    solar power stock crash friday solar hot suddenli\n",
            "16411    rosen top rank firm remind yayyo inc investor ...\n",
            "16412    facebook block new polit ad may fall short sti...\n",
            "Name: Article, Length: 16413, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v_A1300jfArI",
        "colab": {}
      },
      "source": [
        "X.to_csv (r'Tech.csv', index = False, header=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HmOB68OSfArK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "e0d03e5f-3c89-40bd-9bd9-58b9231e8029"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "batch=256\n",
        "#TEXT = data.Field(tokenize='spacy',batch_first=True,include_lengths=True)\n",
        "TEXT = data.Field(batch_first=True,include_lengths=True)\n",
        "LABEL = data.LabelField(dtype = torch.long,batch_first=True)\n",
        "\n",
        "fields = [('text',TEXT),('label', LABEL)]\n",
        "Train_Data=data.TabularDataset(path = 'Tech.csv',format = 'csv',fields = fields,skip_header = True)\n",
        "\n",
        "X_train, X_test = Train_Data.split(split_ratio=0.75, random_state = random.seed(1234))\n",
        "X_train, X_val = X_train.split(split_ratio=0.8, random_state = random.seed(1234))\n",
        "\n",
        "TEXT.build_vocab(X_train, min_freq=2)  \n",
        "LABEL.build_vocab(X_train)\n",
        "\n",
        "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
        "\n",
        "#No. of unique tokens in label\n",
        "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
        "\n",
        "#Commonly used words\n",
        "print(TEXT.vocab.freqs.most_common(10))  \n",
        "train_it, val_it, test_it = data.BucketIterator.splits((X_train, X_val, X_test),sort_key = lambda x: len(x.text),\n",
        "    sort_within_batch=True,batch_size = batch,device = device)  \n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of TEXT vocabulary: 7975\n",
            "Size of LABEL vocabulary: 3\n",
            "[('stock', 4315), ('earn', 3794), ('inc', 2794), ('result', 2505), ('compani', 2471), ('announc', 2258), ('report', 2024), ('busi', 1997), ('quarter', 1981), ('new', 1578)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hIvY8I8tfArM",
        "colab": {}
      },
      "source": [
        "def find_accuracy(preds, y):\n",
        "   \n",
        "    temp = torch.log_softmax(preds, dim = 1)\n",
        "    _, y_pred = torch.max(temp, dim = 1) \n",
        "    valid = (y_pred == y).float() \n",
        "    accur = valid.sum() / len(valid)\n",
        "    return accur\n",
        "\n",
        "def Loss_Optimizer (model,valueLR):\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=valueLR)\n",
        "    return optimizer,criterion"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lNSC2cF4fArO",
        "colab": {}
      },
      "source": [
        "def train(model,data,lr,optimizer,criterion):\n",
        "        \n",
        "    #optimizer,criterion=Loss_Optimizer (model,lr)\n",
        "    #criterion = criterion.to(device)\n",
        "    model.train()\n",
        "    sumloss=0.0\n",
        "    sumacc=0.0\n",
        "    for i in data:\n",
        "        text, text_lengths = i.text   \n",
        "        optimizer.zero_grad()\n",
        "        pred = model(text, text_lengths).squeeze(0)       \n",
        "        loss = criterion(pred, i.label)\n",
        "        acc = find_accuracy(pred, i.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        sumloss += loss.item()\n",
        "        sumacc += acc.item()\n",
        "          \n",
        "    return sumloss / len(data), sumacc / len(data)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bqLTVN39fArR",
        "colab": {}
      },
      "source": [
        "def test(model, data, criterion):\n",
        "    \n",
        "    sumloss = 0\n",
        "    sumacc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i in data:\n",
        "            text, text_lengths = i.text\n",
        "            \n",
        "            pred = model(text, text_lengths).squeeze(0)\n",
        "           \n",
        "            loss = criterion(pred, i.label)\n",
        "            \n",
        "            acc = find_accuracy(pred, i.label)\n",
        "\n",
        "            sumloss += loss.item()\n",
        "            sumacc += acc.item()\n",
        "        \n",
        "    return sumloss / len(data), sumacc / len(data)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GnTkljWVfArT",
        "colab": {}
      },
      "source": [
        "def process_test(model,numEpochs,data_tr,data_val,data_test,lr,loss_file,acc_file,best_model):\n",
        "\n",
        "    optimizer,criterion=Loss_Optimizer (model,lr)\n",
        "    criterion = criterion.to(device)\n",
        "    start_time = time.time()\n",
        "    fileout=open(loss_file,\"w\")\n",
        "    fileout2=open(acc_file,\"w\")\n",
        "    \n",
        "    for i in range(numEpochs):\n",
        " \n",
        "        train_loss, train_acc = train(model, data_tr, lr,optimizer,criterion)\n",
        "        valid_loss, valid_acc = test(model, data_val, criterion)\n",
        "        fileout.write(str(valid_loss)+\"\\n\")\n",
        "        fileout2.write(str(valid_acc)+\"\\n\")\n",
        "        if i==0:\n",
        "            max=valid_acc\n",
        "            torch.save(model.state_dict(), best_model)\n",
        "        elif valid_acc > max:\n",
        "            max = valid_acc\n",
        "            torch.save(model.state_dict(), best_model)\n",
        "\n",
        "        if (i+1)%5==0:\n",
        "          print(\"Epoch : \",i+1,\" Train Loss : \",train_loss,\"  Train Acc : \",train_acc,\"  Valid Loss : \",valid_loss, \" Val Acc : \",valid_acc)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    timeHelp=(end_time-start_time)/60.0\n",
        "    print(\"\\nTime needed for Training : \",timeHelp)\n",
        "\n",
        "    fileout.close()\n",
        "    fileout2.close()  \n",
        "    \n",
        "   \n",
        "    model.load_state_dict(torch.load(best_model))\n",
        "    \n",
        "    test_loss, test_acc = test(model, data_test, criterion)\n",
        "    print(\"\\nLoss in Testset : \",test_loss,\"  Accuracy in Testset : \",test_acc,\"\\n\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hLSNEK-dfArU",
        "colab": {}
      },
      "source": [
        "def process_test2(model,numEpochs,data_tr,data_val,data_test,lr,loss_file,acc_file,best_model):\n",
        "\n",
        "    optimizer,criterion=Loss_Optimizer (model,lr)\n",
        "    criterion = criterion.to(device)\n",
        "    start_time = time.time()\n",
        "    fileout=open(loss_file,\"w\")\n",
        "    fileout2=open(acc_file,\"w\")\n",
        "    \n",
        "    for i in range(numEpochs):\n",
        " \n",
        "        train_loss, train_acc = train(model, data_tr, lr,optimizer,criterion)\n",
        "        valid_loss, valid_acc = test(model, data_val, criterion)\n",
        "        fileout.write(str(valid_loss)+\"\\n\")\n",
        "        fileout2.write(str(valid_acc)+\"\\n\")\n",
        "        if i==0:\n",
        "            minloss=valid_loss\n",
        "            torch.save(model.state_dict(), best_model)\n",
        "        elif valid_loss < minloss:\n",
        "            minloss = valid_loss\n",
        "            torch.save(model.state_dict(), best_model)\n",
        "\n",
        "        if (i+1)%5==0:\n",
        "          print(\"Epoch : \",i+1,\" Train Loss : \",train_loss,\"  Train Acc : \",train_acc,\"  Valid Loss : \",valid_loss, \" Val Acc : \",valid_acc)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    timeHelp=(end_time-start_time)/60.0\n",
        "    print(\"\\nTime needed for Training : \",timeHelp)\n",
        "\n",
        "    fileout.close()\n",
        "    fileout2.close()  \n",
        "    \n",
        "   \n",
        "    model.load_state_dict(torch.load(best_model))\n",
        "    \n",
        "    test_loss, test_acc = test(model, data_test, criterion)\n",
        "    print(\"\\nLoss in Testset : \",test_loss,\"  Accuracy in Testset : \",test_acc,\"\\n\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OTVfTSDHfArW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ceea007a-ec70-4b57-9d15-0ced3b38722a"
      },
      "source": [
        "print(\"GPU Model   \",torch.cuda.get_device_name(0))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU Model    Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iN7ov3gHfArY",
        "colab": {}
      },
      "source": [
        "class myLSTM(nn.Module):\n",
        "    def __init__(self, voc, embed_d, hid_d, out_d,dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(voc, embed_d)\n",
        "        self.rnn = nn.LSTM(embed_d,hid_d,batch_first=True,num_layers=2,bidirectional=True,dropout=dropout)\n",
        "        self.fc = nn.Linear(hid_d * 2, out_d)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, input_text, text_lengths):\n",
        "                \n",
        "        embedded = self.dropout(self.embedding(input_text))\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n",
        "        rnn_out,(rnn_hid,rnn_cell) = self.rnn(packed_embedded)\n",
        "        hidden = self.dropout(torch.cat((rnn_hid[-2,:,:], rnn_hid[-1,:,:]), dim=1))\n",
        "        return self.fc(hidden)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4OCyMqTxfArb",
        "colab": {}
      },
      "source": [
        "class myGRU(nn.Module):\n",
        "    def __init__(self, voc, embed_d, hid_d, out_d,dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(voc, embed_d)\n",
        "        self.rnn = nn.GRU(embed_d,hid_d,num_layers=2,bidirectional=True,dropout=dropout)\n",
        "        self.fc = nn.Linear(hid_d * 2, out_d)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, input_text,text_lengths):\n",
        "                \n",
        "        embedded = self.dropout(self.embedding(input_text))\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n",
        "        rnn_out,hidden = self.rnn(packed_embedded)\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "        return self.fc(hidden)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tMbHHUnifArd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "55ef14c3-d126-4311-b95d-d465f5230c04"
      },
      "source": [
        "myInput = len(TEXT.vocab)\n",
        "myEmbed = 300\n",
        "myHid = 256\n",
        "myOut = 3\n",
        "dropout = 0.4\n",
        "\n",
        "new_model2 = myLSTM(myInput, myEmbed, myHid, myOut,dropout)\n",
        "new_model2.to(torch.device(device))\n",
        "new_model3 = myGRU(myInput, myEmbed, myHid, myOut,dropout)\n",
        "new_model3.to(torch.device(device))\n",
        "\n",
        "print(new_model2)\n",
        "print(new_model3)\n",
        "#torch.save(new_model2.state_dict(), 'model_LSTM_1.pt')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "myLSTM(\n",
            "  (embedding): Embedding(7975, 300)\n",
            "  (rnn): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
            "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
            "  (dropout): Dropout(p=0.4, inplace=False)\n",
            ")\n",
            "myGRU(\n",
            "  (embedding): Embedding(7975, 300)\n",
            "  (rnn): GRU(300, 256, num_layers=2, dropout=0.4, bidirectional=True)\n",
            "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
            "  (dropout): Dropout(p=0.4, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xEMSmzBOfArf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "34f034be-5c6e-472b-e177-5273da85cb0f"
      },
      "source": [
        "process_test(new_model2,30,train_it,val_it,test_it,0.001,\"lstm1_loss.txt\",\"lstm1_acc.txt\",\"best_LSTM_1_model.pt\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch :  5  Train Loss :  0.2446557925775265   Train Acc :  0.850761217948718   Valid Loss :  0.3739458488300443  Val Acc :  0.7955152332782746\n",
            "Epoch :  10  Train Loss :  0.08918671423975283   Train Acc :  0.9504206730769231   Valid Loss :  0.5475802920758724  Val Acc :  0.8250543951988221\n",
            "Epoch :  15  Train Loss :  0.055156186141688615   Train Acc :  0.9712540064102564   Valid Loss :  0.7142435094341636  Val Acc :  0.814898145198822\n",
            "Epoch :  20  Train Loss :  0.03234899217018093   Train Acc :  0.9811698717948718   Valid Loss :  0.8090851616114378  Val Acc :  0.8193433582782745\n",
            "Epoch :  25  Train Loss :  0.021558110030850796   Train Acc :  0.9876802884615384   Valid Loss :  0.9964068859815598  Val Acc :  0.8240308582782745\n",
            "Epoch :  30  Train Loss :  0.015275980678196879   Train Acc :  0.9925881410256411   Valid Loss :  0.9893509875983  Val Acc :  0.8318433582782745\n",
            "\n",
            "Time needed for Training :  0.7559653997421265\n",
            "\n",
            "Loss in Testset :  1.0086010599847857   Accuracy in Testset :  0.8403033088235294 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ufEdn9rhfAri",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "fc85e524-ad4d-4240-9ae5-60003c2b2fd7"
      },
      "source": [
        "process_test(new_model3,30,train_it,val_it,test_it,0.001,\"gru1_loss.txt\",\"gru1_acc.txt\",\"best_gru_1_model.pt\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch :  5  Train Loss :  0.2660508098032994   Train Acc :  0.8358373397435898   Valid Loss :  0.3731095362454653  Val Acc :  0.7974683582782746\n",
            "Epoch :  10  Train Loss :  0.12540060491898122   Train Acc :  0.9294871794871795   Valid Loss :  0.5256267599761486  Val Acc :  0.8209058582782746\n",
            "Epoch :  15  Train Loss :  0.06156524823596462   Train Acc :  0.9652443910256411   Valid Loss :  0.6619195275008678  Val Acc :  0.8216871082782745\n",
            "Epoch :  20  Train Loss :  0.03784453595737712   Train Acc :  0.9808693910256411   Valid Loss :  0.7827127873897552  Val Acc :  0.8316010713577271\n",
            "Epoch :  25  Train Loss :  0.030046154589702685   Train Acc :  0.9845753205128205   Valid Loss :  0.9279721066355705  Val Acc :  0.8369214832782745\n",
            "Epoch :  30  Train Loss :  0.029015343214082234   Train Acc :  0.9845753205128205   Valid Loss :  0.8717227898538112  Val Acc :  0.8296479463577271\n",
            "\n",
            "Time needed for Training :  0.6846999367078145\n",
            "\n",
            "Loss in Testset :  0.8793901357513438   Accuracy in Testset :  0.8357077205882353 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}