{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Tech_Rnn_(1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l9o-8aJZgk9I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "9a1b1a6a-19e1-4115-d8a4-eae7210cf9a4"
      },
      "source": [
        ">>> import nltk\n",
        ">>> nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a5HnPotefAq7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d9e47855-315c-46dd-fea3-eb2409449c2e"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torchtext import data\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import spacy\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"GPU Model   \",torch.cuda.get_device_name(0))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU Model    Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VCJKt_MyfAq-",
        "colab": {}
      },
      "source": [
        "def clean_data(X):\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "    X=X.str.lower()\n",
        "    X=X.str.replace(\"[/(){}\\[\\]\\|@,;]\", \" \")\n",
        "    X=X.str.replace(\"[^0-9a-z #+_]\", \" \")\n",
        "    X = X.str.replace(r'\\d+','')\n",
        "    X = X.apply(lambda x: ' '.join([w for w in str(x).split() if (len(w)>2 and w not in STOPWORDS) ] ))\n",
        "    X = X.apply(lambda x: x.split()) \n",
        "    return X\n",
        "\n",
        "def target_arrange(y):\n",
        "    \n",
        "    for i in range(len(y)):\n",
        "        if y.values[i]==\"Negative\":\n",
        "            y.values[i]=0.0\n",
        "        elif y[i]==\"Positive\":\n",
        "            y.values[i]=1.0\n",
        "        else:\n",
        "            y.values[i]=2.0\n",
        "            \n",
        "    y=y.to_numpy()  \n",
        "    y=y.reshape(y.shape[0],1)\n",
        "    y= pd.DataFrame(data=y)\n",
        "    y=np.ravel(y)\n",
        "    y=y.astype('float')\n",
        "    return y"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "39r_65tmfArB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "34eda083-796b-4daf-d929-83fc2b0368d8"
      },
      "source": [
        "with open(\"Tech_news.json\",\"r\") as fp:\n",
        "#with open(\"General_Market.json\",encoding='utf8') as fp:\n",
        "    json_d = json.load(fp)\n",
        " \n",
        "ticks_d = json_d['data']\n",
        "df = pd.DataFrame(ticks_d)\n",
        "\n",
        "X= pd.DataFrame(columns=['Date', 'Article','Target'])\n",
        "X['Date']=pd.to_datetime(df['date'])\n",
        "X['Article']=df['title']+\" \"+df['text']\n",
        "X['Target']=df['sentiment']\n",
        "\n",
        "X=X.sort_values(\"Date\")\n",
        "\n",
        "print(\"Number of Examples : \",len(X),\"\\n\")\n",
        "X.drop_duplicates(inplace=True)\n",
        "X.index = range(len(X))\n",
        "print(\"Number of Examples after removing duplicates: \",len(X),\"\\n\")\n",
        "\n",
        "print('Number of words before cleaning : ',X['Article'].apply(lambda x: len(str(x).split(' '))).sum())\n",
        "X['Article']=clean_data(X['Article'])\n",
        "print('Number of words after cleaning : ',X['Article'].apply(lambda x: len(str(x).split(' '))).sum())\n",
        "print(\"\\n******************\\n\")\n",
        "#print(X.groupby(['Target']).count())\n",
        "X['Target']=target_arrange(X['Target'])\n",
        "X=X.drop('Date',1)\n",
        "\n",
        "L=[]\n",
        "L.append((X['Target']==0.0).sum())\n",
        "L.append((X['Target']==1.0).sum())\n",
        "L.append((X['Target']==2.0).sum())\n",
        "\n",
        "print(\"Negative Examples : \",(X['Target']==0.0).sum())\n",
        "print(\"Positive Examples : \",(X['Target']==1.0).sum())\n",
        "print(\"Neutral Examples : \",(X['Target']==2.0).sum())\n",
        "\n",
        "maximum=max(L)\n",
        "\n",
        "Weights=[]\n",
        "\n",
        "for i in L:\n",
        "  Weights.append(maximum/i)\n",
        "  \n",
        "class_weights = torch.FloatTensor(Weights).to(device)\n",
        "print(\"\\n Weights = \",class_weights)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Examples :  15641 \n",
            "\n",
            "Number of Examples after removing duplicates:  15432 \n",
            "\n",
            "Number of words before cleaning :  585553\n",
            "Number of words after cleaning :  393507\n",
            "\n",
            "******************\n",
            "\n",
            "Negative Examples :  1892\n",
            "Positive Examples :  5838\n",
            "Neutral Examples :  7702\n",
            "\n",
            " Weights =  tensor([4.0708, 1.3193, 1.0000], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F2Lyyf7vfArF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "a0b8c6be-0f59-4762-b3e5-b207272e2f13"
      },
      "source": [
        "X['Article']= X['Article'].apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
        "for i in range(len(X['Article'])): #φέρνω τα tokens ξανά μαζί διαχωριζόμενα με κενά\n",
        "    X['Article'][i] = ' '.join(X['Article'][i])\n",
        "print(X['Article'])\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0        mercuri system receiv contract award base new ...\n",
            "1        progress second quarter revenu exce guidanc in...\n",
            "2        acuiti brand declar quarterli dividend atlanta...\n",
            "3         share factset soar today earn came better expect\n",
            "4        stifel say inseego leader inseego corp nasdaq ...\n",
            "                               ...                        \n",
            "15427    lead plaintiff deadlin alert faruqi faruqi llp...\n",
            "15428    taiwan semiconductor stock best chip make game...\n",
            "15429    twilio stock rock star may done yet twilio sto...\n",
            "15430    incred stat zoom follow blowout earn report zo...\n",
            "15431    explor sale xandr advertis unit amid restructu...\n",
            "Name: Article, Length: 15432, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v_A1300jfArI",
        "colab": {}
      },
      "source": [
        "X.to_csv (r'Tech.csv', index = False, header=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HmOB68OSfArK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "c54f8dba-f295-45a6-913d-7fe1d45018b5"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "batch=256\n",
        "#TEXT = data.Field(tokenize='spacy',batch_first=True,include_lengths=True)\n",
        "TEXT = data.Field(batch_first=True,include_lengths=True)\n",
        "LABEL = data.LabelField(dtype = torch.long,batch_first=True)\n",
        "\n",
        "fields = [('text',TEXT),('label', LABEL)]\n",
        "Train_Data=data.TabularDataset(path = 'Tech.csv',format = 'csv',fields = fields,skip_header = True)\n",
        "\n",
        "X_train, X_test = Train_Data.split(split_ratio=0.7, random_state = random.seed(1234))\n",
        "X_train, X_val = X_train.split(split_ratio=0.8, random_state = random.seed(1234))\n",
        "\n",
        "TEXT.build_vocab(X_train, min_freq=2)  \n",
        "LABEL.build_vocab(X_train)\n",
        "\n",
        "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
        "\n",
        "#No. of unique tokens in label\n",
        "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
        "\n",
        "#Commonly used words\n",
        "print(TEXT.vocab.freqs.most_common(10))  \n",
        "train_it, val_it, test_it = data.BucketIterator.splits((X_train, X_val, X_test),sort_key = lambda x: len(x.text),\n",
        "    sort_within_batch=True,batch_size = batch,device = device)  \n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of TEXT vocabulary: 7604\n",
            "Size of LABEL vocabulary: 3\n",
            "[('stock', 3765), ('earn', 3275), ('inc', 2413), ('result', 2250), ('compani', 2174), ('announc', 2041), ('report', 1817), ('quarter', 1812), ('busi', 1726), ('technolog', 1328)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hIvY8I8tfArM",
        "colab": {}
      },
      "source": [
        "def find_accuracy(preds, y):\n",
        "   \n",
        "    temp = torch.log_softmax(preds, dim = 1)\n",
        "    _, y_pred = torch.max(temp, dim = 1) \n",
        "    valid = (y_pred == y).float() \n",
        "    accur = valid.sum() / len(valid)\n",
        "    return accur\n",
        "\n",
        "def Loss_Optimizer (model,valueLR):\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=valueLR)\n",
        "    return optimizer,criterion"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lNSC2cF4fArO",
        "colab": {}
      },
      "source": [
        "def train(model,data,lr,optimizer,criterion):\n",
        "        \n",
        "    #optimizer,criterion=Loss_Optimizer (model,lr)\n",
        "    #criterion = criterion.to(device)\n",
        "    model.train()\n",
        "    sumloss=0.0\n",
        "    sumacc=0.0\n",
        "    for i in data:\n",
        "        text, text_lengths = i.text   \n",
        "        optimizer.zero_grad()\n",
        "        pred = model(text, text_lengths).squeeze(0)       \n",
        "        loss = criterion(pred, i.label)\n",
        "        acc = find_accuracy(pred, i.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        sumloss += loss.item()\n",
        "        sumacc += acc.item()\n",
        "          \n",
        "    return sumloss / len(data), sumacc / len(data)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bqLTVN39fArR",
        "colab": {}
      },
      "source": [
        "def test(model, data, criterion):\n",
        "    \n",
        "    sumloss = 0\n",
        "    sumacc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i in data:\n",
        "            text, text_lengths = i.text\n",
        "            \n",
        "            pred = model(text, text_lengths).squeeze(0)\n",
        "           \n",
        "            loss = criterion(pred, i.label)\n",
        "            \n",
        "            acc = find_accuracy(pred, i.label)\n",
        "\n",
        "            sumloss += loss.item()\n",
        "            sumacc += acc.item()\n",
        "        \n",
        "    return sumloss / len(data), sumacc / len(data)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GnTkljWVfArT",
        "colab": {}
      },
      "source": [
        "def process_test(model,numEpochs,data_tr,data_val,data_test,lr,loss_file,acc_file,best_model):\n",
        "\n",
        "    optimizer,criterion=Loss_Optimizer (model,lr)\n",
        "    criterion = criterion.to(device)\n",
        "    start_time = time.time()\n",
        "    fileout=open(loss_file,\"w\")\n",
        "    fileout2=open(acc_file,\"w\")\n",
        "    \n",
        "    for i in range(numEpochs):\n",
        " \n",
        "        train_loss, train_acc = train(model, data_tr, lr,optimizer,criterion)\n",
        "        valid_loss, valid_acc = test(model, data_val, criterion)\n",
        "        fileout.write(str(valid_loss)+\"\\n\")\n",
        "        fileout2.write(str(valid_acc)+\"\\n\")\n",
        "        if i==0:\n",
        "            max=valid_acc\n",
        "            torch.save(model.state_dict(), best_model)\n",
        "        elif valid_acc > max:\n",
        "            max = valid_acc\n",
        "            torch.save(model.state_dict(), best_model)\n",
        "\n",
        "        if (i+1)%5==0:\n",
        "          print(\"Epoch : \",i+1,\" Train Loss : \",train_loss,\"  Train Acc : \",train_acc,\"  Valid Loss : \",valid_loss, \" Val Acc : \",valid_acc)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    timeHelp=(end_time-start_time)/60.0\n",
        "    print(\"\\nTime needed for Training : \",timeHelp)\n",
        "\n",
        "    fileout.close()\n",
        "    fileout2.close()  \n",
        "    \n",
        "   \n",
        "    model.load_state_dict(torch.load(best_model))\n",
        "    \n",
        "    test_loss, test_acc = test(model, data_test, criterion)\n",
        "    print(\"\\nLoss in Testset : \",test_loss,\"  Accuracy in Testset : \",test_acc,\"\\n\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hLSNEK-dfArU",
        "colab": {}
      },
      "source": [
        "def process_test2(model,numEpochs,data_tr,data_val,data_test,lr,loss_file,acc_file,best_model):\n",
        "\n",
        "    optimizer,criterion=Loss_Optimizer (model,lr)\n",
        "    criterion = criterion.to(device)\n",
        "    start_time = time.time()\n",
        "    fileout=open(loss_file,\"w\")\n",
        "    fileout2=open(acc_file,\"w\")\n",
        "    \n",
        "    for i in range(numEpochs):\n",
        " \n",
        "        train_loss, train_acc = train(model, data_tr, lr,optimizer,criterion)\n",
        "        valid_loss, valid_acc = test(model, data_val, criterion)\n",
        "        fileout.write(str(valid_loss)+\"\\n\")\n",
        "        fileout2.write(str(valid_acc)+\"\\n\")\n",
        "        if i==0:\n",
        "            minloss=valid_loss\n",
        "            torch.save(model.state_dict(), best_model)\n",
        "        elif valid_loss < minloss:\n",
        "            minloss = valid_loss\n",
        "            torch.save(model.state_dict(), best_model)\n",
        "\n",
        "        if (i+1)%5==0:\n",
        "          print(\"Epoch : \",i+1,\" Train Loss : \",train_loss,\"  Train Acc : \",train_acc,\"  Valid Loss : \",valid_loss, \" Val Acc : \",valid_acc)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    timeHelp=(end_time-start_time)/60.0\n",
        "    print(\"\\nTime needed for Training : \",timeHelp)\n",
        "\n",
        "    fileout.close()\n",
        "    fileout2.close()  \n",
        "    \n",
        "   \n",
        "    model.load_state_dict(torch.load(best_model))\n",
        "    \n",
        "    test_loss, test_acc = test(model, data_test, criterion)\n",
        "    print(\"\\nLoss in Testset : \",test_loss,\"  Accuracy in Testset : \",test_acc,\"\\n\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OTVfTSDHfArW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60d4d8ea-bb14-40c5-d8ba-fde30ce1fa1c"
      },
      "source": [
        "print(\"GPU Model   \",torch.cuda.get_device_name(0))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU Model    Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iN7ov3gHfArY",
        "colab": {}
      },
      "source": [
        "class myLSTM(nn.Module):\n",
        "    def __init__(self, voc, embed_d, hid_d, out_d,dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(voc, embed_d)\n",
        "        self.rnn = nn.LSTM(embed_d,hid_d,batch_first=True,num_layers=2,bidirectional=True,dropout=dropout)\n",
        "        self.fc = nn.Linear(hid_d * 2, out_d)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, input_text, text_lengths):\n",
        "                \n",
        "        embedded = self.dropout(self.embedding(input_text))\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n",
        "        rnn_out,(rnn_hid,rnn_cell) = self.rnn(packed_embedded)\n",
        "        hidden = self.dropout(torch.cat((rnn_hid[-2,:,:], rnn_hid[-1,:,:]), dim=1))\n",
        "        return self.fc(hidden)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4OCyMqTxfArb",
        "colab": {}
      },
      "source": [
        "class myGRU(nn.Module):\n",
        "    def __init__(self, voc, embed_d, hid_d, out_d,dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(voc, embed_d)\n",
        "        self.rnn = nn.GRU(embed_d,hid_d,num_layers=2,bidirectional=True,dropout=dropout)\n",
        "        self.fc = nn.Linear(hid_d * 2, out_d)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, input_text,text_lengths):\n",
        "                \n",
        "        embedded = self.dropout(self.embedding(input_text))\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n",
        "        rnn_out,hidden = self.rnn(packed_embedded)\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "        return self.fc(hidden)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tMbHHUnifArd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "80896e9e-2729-4b69-be3f-d4a4f2159687"
      },
      "source": [
        "myInput = len(TEXT.vocab)\n",
        "myEmbed = 300\n",
        "myHid = 256\n",
        "myOut = 3\n",
        "dropout = 0.4\n",
        "\n",
        "new_model2 = myLSTM(myInput, myEmbed, myHid, myOut,dropout)\n",
        "new_model2.to(torch.device(device))\n",
        "new_model3 = myGRU(myInput, myEmbed, myHid, myOut,dropout)\n",
        "new_model3.to(torch.device(device))\n",
        "\n",
        "print(new_model2)\n",
        "print(new_model3)\n",
        "#torch.save(new_model2.state_dict(), 'model_LSTM_1.pt')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "myLSTM(\n",
            "  (embedding): Embedding(7604, 300)\n",
            "  (rnn): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
            "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
            "  (dropout): Dropout(p=0.4, inplace=False)\n",
            ")\n",
            "myGRU(\n",
            "  (embedding): Embedding(7604, 300)\n",
            "  (rnn): GRU(300, 256, num_layers=2, dropout=0.4, bidirectional=True)\n",
            "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
            "  (dropout): Dropout(p=0.4, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xEMSmzBOfArf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "0adad904-3b04-41fa-b417-4a989c946ed2"
      },
      "source": [
        "process_test(new_model2,30,train_it,val_it,test_it,0.001,\"lstm1_loss.txt\",\"lstm1_acc.txt\",\"best_LSTM_1_model.pt\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch :  5  Train Loss :  0.2576934501857442   Train Acc :  0.8417968732469222   Valid Loss :  0.43743111689885456  Val Acc :  0.8064236111111112\n",
            "Epoch :  10  Train Loss :  0.10272306690732126   Train Acc :  0.9443933805998634   Valid Loss :  0.5944082545013063  Val Acc :  0.8093378014034696\n",
            "Epoch :  15  Train Loss :  0.0417797258718672   Train Acc :  0.9772518364822164   Valid Loss :  0.83327941223979  Val Acc :  0.8097718291812472\n",
            "Epoch :  20  Train Loss :  0.032135021476064785   Train Acc :  0.9828814320704516   Valid Loss :  0.9649006779719558  Val Acc :  0.8058655791812472\n",
            "Epoch :  25  Train Loss :  0.01857244676177609   Train Acc :  0.990808821776334   Valid Loss :  1.0699947975162003  Val Acc :  0.818018356959025\n",
            "Epoch :  30  Train Loss :  0.022855999606476603   Train Acc :  0.9889705864822164   Valid Loss :  0.9904275880609121  Val Acc :  0.8089037736256918\n",
            "\n",
            "Time needed for Training :  0.7764581680297852\n",
            "\n",
            "Loss in Testset :  1.0194450191369182   Accuracy in Testset :  0.8333395656786466 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ufEdn9rhfAri",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "0245f529-509f-489c-c042-8544c082c1d2"
      },
      "source": [
        "process_test(new_model3,30,train_it,val_it,test_it,0.001,\"gru1_loss.txt\",\"gru1_acc.txt\",\"best_gru_1_model.pt\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch :  5  Train Loss :  0.2738383040379952   Train Acc :  0.8347886011880987   Valid Loss :  0.39098928175452685  Val Acc :  0.8033854166666666\n",
            "Epoch :  10  Train Loss :  0.12625998173675992   Train Acc :  0.9303768364822164   Valid Loss :  0.5452661157921992  Val Acc :  0.8090277777777778\n",
            "Epoch :  15  Train Loss :  0.05979759281720309   Train Acc :  0.9657261494327994   Valid Loss :  0.7841192494185331  Val Acc :  0.8229166666666666\n",
            "Epoch :  20  Train Loss :  0.04976228163029779   Train Acc :  0.975183821776334   Valid Loss :  0.7608811949224522  Val Acc :  0.8198784722222222\n",
            "Epoch :  25  Train Loss :  0.028087836438156526   Train Acc :  0.9840303291292751   Valid Loss :  0.9980696097546671  Val Acc :  0.8203125\n",
            "Epoch :  30  Train Loss :  0.02240768766034093   Train Acc :  0.9883961379528046   Valid Loss :  1.0199691245729303  Val Acc :  0.8203125\n",
            "\n",
            "Time needed for Training :  0.6738803466161092\n",
            "\n",
            "Loss in Testset :  0.8052080297646554   Accuracy in Testset :  0.8248355263157895 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}