{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection,metrics,naive_bayes,preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "stemmer = PorterStemmer()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(X):\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    X=X.str.lower()\n",
    "    X=X.str.replace(\"[/(){}\\[\\]\\|@,;]\", \" \")\n",
    "    X=X.str.replace(\"[^0-9a-z #+_]\", \" \")\n",
    "    X = X.str.replace(r'\\d+','')\n",
    "    X = X.apply(lambda x: ' '.join([w for w in str(x).split() if (len(w)>2 and w not in STOPWORDS) ] ))\n",
    "    X = X.apply(lambda x: x.split()) \n",
    "    return X\n",
    "\n",
    "def target_arrange(y):\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        if y.values[i]==\"Negative\":\n",
    "            y.values[i]=0.0\n",
    "        elif y[i]==\"Positive\":\n",
    "            y.values[i]=1.0\n",
    "        else:\n",
    "            y.values[i]=2.0\n",
    "            \n",
    "    y=y.to_numpy()  \n",
    "    y=y.reshape(y.shape[0],1)\n",
    "    y= pd.DataFrame(data=y)\n",
    "    y=np.ravel(y)\n",
    "    y=y.astype('float')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Examples :  16651 \n",
      "\n",
      "Number of Examples after removing duplicates:  16413 \n",
      "\n",
      "Number of words before cleaning :  619298\n",
      "Number of words after cleaning :  416411\n",
      "          Date  Article\n",
      "Target                 \n",
      "Negative  2040     2040\n",
      "Neutral   8178     8178\n",
      "Positive  6195     6195\n"
     ]
    }
   ],
   "source": [
    "with open(\"Tech_news.json\",\"r\") as fp:\n",
    "#with open(\"General_Market.json\",encoding='utf8') as fp:\n",
    "    json_d = json.load(fp)\n",
    " \n",
    "ticks_d = json_d['data']\n",
    "df = pd.DataFrame(ticks_d)\n",
    "\n",
    "X= pd.DataFrame(columns=['Date', 'Article','Target'])\n",
    "X['Date']=pd.to_datetime(df['date'])\n",
    "X['Article']=df['title']+\" \"+df['text']\n",
    "X['Target']=df['sentiment']\n",
    "\n",
    "X=X.sort_values(\"Date\")\n",
    "print(\"Number of Examples : \",len(X),\"\\n\")\n",
    "X.drop_duplicates(inplace=True)\n",
    "X.index = range(len(X))\n",
    "print(\"Number of Examples after removing duplicates: \",len(X),\"\\n\")\n",
    "X.to_csv (r'Tech.csv', index = False, header=True)\n",
    "\n",
    "print('Number of words before cleaning : ',X['Article'].apply(lambda x: len(str(x).split(' '))).sum())\n",
    "X['Article']=clean_data(X['Article'])\n",
    "print('Number of words after cleaning : ',X['Article'].apply(lambda x: len(str(x).split(' '))).sum())\n",
    "\n",
    "print(X.groupby(['Target']).count())\n",
    "\n",
    "y=target_arrange(X['Target'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       2020-06-25 16:15:00-04:00\n",
      "1       2020-06-25 16:15:00-04:00\n",
      "2       2020-06-25 16:15:00-04:00\n",
      "3       2020-06-25 16:18:00-04:00\n",
      "4       2020-06-25 16:21:36-04:00\n",
      "                   ...           \n",
      "16408   2020-09-04 14:03:00-04:00\n",
      "16409   2020-09-04 14:03:00-04:00\n",
      "16410   2020-09-04 14:19:00-04:00\n",
      "16411   2020-09-04 14:33:00-04:00\n",
      "16412   2020-09-04 14:33:55-04:00\n",
      "Name: Date, Length: 16413, dtype: datetime64[ns, pytz.FixedOffset(-240)]    0        acuiti brand declar quarterli dividend atlanta...\n",
      "1        progress second quarter revenu exce guidanc in...\n",
      "2        mercuri system receiv contract award base new ...\n",
      "3         share factset soar today earn came better expect\n",
      "4        stifel say inseego leader inseego corp nasdaq ...\n",
      "                               ...                        \n",
      "16408    adob stock fall today investor take profit mad...\n",
      "16409    dow jone drop point appl salesforc stock head ...\n",
      "16410    solar power stock crash friday solar hot suddenli\n",
      "16411    rosen top rank firm remind yayyo inc investor ...\n",
      "16412    facebook block new polit ad may fall short sti...\n",
      "Name: Article, Length: 16413, dtype: object\n"
     ]
    }
   ],
   "source": [
    "X['Article']= X['Article'].apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "for i in range(len(X['Article'])): #φέρνω τα tokens ξανά μαζί διαχωριζόμενα με κενά\n",
    "    X['Article'][i] = ' '.join(X['Article'][i])\n",
    "\n",
    "print(X['Date'],\"  \",X['Article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9)\n",
    "Xv = tfidf_vectorizer.fit_transform(X['Article'])\n",
    "Xv = pd.DataFrame(Xv.todense())\n",
    "X_train,X_test,y_train,y_test = train_test_split(Xv,y, test_size=0.25,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Recall metric: 0.704794073043521\n",
      " F1 metric: 0.7104724016741927\n",
      " Precision metric: 0.727225350061271\n",
      " Accuracy metric: 0.7655945419103314\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1 # This is the smoothing parameter for Laplace/Lidstone smoothing\n",
    "model = naive_bayes.MultinomialNB(alpha=alpha)\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "\n",
    "recall = metrics.recall_score(y_test,y_predicted,average='macro')\n",
    "precision = metrics.precision_score(y_test,y_predicted,average='macro')\n",
    "f1 = metrics.f1_score(y_test,y_predicted,average='macro')\n",
    "Accur=metrics.accuracy_score(y_test,y_predicted)\n",
    "\n",
    "print(' Recall metric:',recall)\n",
    "print(' F1 metric:',f1)\n",
    "print(' Precision metric:',precision)\n",
    "print(' Accuracy metric:',Accur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7847433540798786\n"
     ]
    }
   ],
   "source": [
    "skfold = model_selection.StratifiedKFold(n_splits=5)\n",
    "model2 = RandomForestClassifier(n_estimators=100, max_features=\"auto\", n_jobs=-1)\n",
    "results = model_selection.cross_val_score(model2, Xv,y, cv=skfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Recall metric: 0.7194772234280703\n",
      " F1 metric: 0.7447079035266074\n",
      " Precision metric: 0.8272809286265966\n",
      " Accuracy metric: 0.8070175438596491\n"
     ]
    }
   ],
   "source": [
    "model2 = RandomForestClassifier(n_estimators=100, max_features=\"auto\", n_jobs=-1)\n",
    "model2.fit(X_train,y_train)\n",
    "y_predicted = model2.predict(X_test)\n",
    "\n",
    "\n",
    "recall = metrics.recall_score(y_test,y_predicted,average='macro')\n",
    "precision = metrics.precision_score(y_test,y_predicted,average='macro')\n",
    "f1 = metrics.f1_score(y_test,y_predicted,average='macro')\n",
    "Accur=metrics.accuracy_score(y_test,y_predicted)\n",
    "\n",
    "print(' Recall metric:',recall)\n",
    "print(' F1 metric:',f1)\n",
    "print(' Precision metric:',precision)\n",
    "print(' Accuracy metric:',Accur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
