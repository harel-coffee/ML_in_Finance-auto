{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "General_Rnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l9o-8aJZgk9I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "6f0033b8-a144-4e09-e09b-29c1b100e8c9"
      },
      "source": [
        ">>> import nltk\n",
        ">>> nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a5HnPotefAq7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ade47ef-efc7-4be7-86da-2e61b240a4c5"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torchtext import data\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import spacy\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"GPU Model   \",torch.cuda.get_device_name(0))\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU Model    Tesla P4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VCJKt_MyfAq-",
        "colab": {}
      },
      "source": [
        "def clean_data(X):\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "    X = X.str.lower()\n",
        "    X = X.str.replace(\"[/(){}\\[\\]\\|@,;]\", \" \")\n",
        "    X = X.str.replace(\"[^0-9a-z #+_]\", \" \")\n",
        "    X = X.str.replace(r'\\d+','')\n",
        "    X = X.apply(lambda x: ' '.join([w for w in x.split() if (len(w)>2 and w not in STOPWORDS) ] ))\n",
        "    X = X.apply(lambda x: x.split()) \n",
        "    return X\n",
        "\n",
        "def target_arrange(y):\n",
        "    \n",
        "    for i in range(len(y)):\n",
        "        if y.values[i]==\"Negative\":\n",
        "            y.values[i]=0.0\n",
        "        elif y[i]==\"Positive\":\n",
        "            y.values[i]=1.0\n",
        "        else:\n",
        "            y.values[i]=2.0\n",
        "            \n",
        "    y=y.to_numpy()  \n",
        "    y=y.reshape(y.shape[0],1)\n",
        "    y= pd.DataFrame(data=y)\n",
        "    y=np.ravel(y)\n",
        "    y=y.astype('float')\n",
        "    return y"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "39r_65tmfArB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "49a7ffbc-4576-4f17-fabd-229f3f53ad80"
      },
      "source": [
        "with open(\"All_Tickers.json\",\"r\") as fp:\n",
        "#with open(\"General_Market.json\",encoding='utf8') as fp:\n",
        "    json_d = json.load(fp)\n",
        " \n",
        "ticks_d = json_d['data']\n",
        "df = pd.DataFrame(ticks_d)\n",
        "\n",
        "X= pd.DataFrame(columns=['Date', 'Article','Target'])\n",
        "X['Date']=pd.to_datetime(df['date'])\n",
        "X['Article']=df['title']+\" \"+df['text']\n",
        "X['Target']=df['sentiment']\n",
        "\n",
        "X=X.sort_values(\"Date\")\n",
        "\n",
        "print(\"Number of Examples : \",len(X),\"\\n\")\n",
        "X.drop_duplicates(inplace=True)\n",
        "X.index = range(len(X))\n",
        "print(\"Number of Examples after removing duplicates: \",len(X),\"\\n\")\n",
        "\n",
        "print('Number of words before cleaning : ',X['Article'].apply(lambda x: len(str(x).split(' '))).sum())\n",
        "X['Article']=clean_data(X['Article'])\n",
        "print('Number of words after cleaning : ',X['Article'].apply(lambda x: len(str(x).split(' '))).sum())\n",
        "print(\"\\n******************\\n\")\n",
        "\n",
        "X['Target']=target_arrange(X['Target'])\n",
        "X=X.drop('Date',1)\n",
        "L=[]\n",
        "L.append((X['Target']==0.0).sum())\n",
        "L.append((X['Target']==1.0).sum())\n",
        "L.append((X['Target']==2.0).sum())\n",
        "\n",
        "print(\"Negative Examples : \",(X['Target']==0.0).sum())\n",
        "print(\"Positive Examples : \",(X['Target']==1.0).sum())\n",
        "print(\"Neutral Examples : \",(X['Target']==2.0).sum())\n",
        "\n",
        "maximum=max(L)\n",
        "\n",
        "Weights=[]\n",
        "\n",
        "for i in L:\n",
        "  Weights.append(maximum/i)\n",
        "  \n",
        "class_weights = torch.FloatTensor(Weights).to(device)\n",
        "print(\"\\n Weights = \",class_weights)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Examples :  24030 \n",
            "\n",
            "Number of Examples after removing duplicates:  23867 \n",
            "\n",
            "Number of words before cleaning :  910682\n",
            "Number of words after cleaning :  612873\n",
            "\n",
            "******************\n",
            "\n",
            "Negative Examples :  2640\n",
            "Positive Examples :  8465\n",
            "Neutral Examples :  12762\n",
            "\n",
            " Weights =  tensor([4.8341, 1.5076, 1.0000], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F2Lyyf7vfArF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "a6b91855-c20b-4371-8cf4-9352a1dacab6"
      },
      "source": [
        "X['Article']= X['Article'].apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
        "for i in range(len(X['Article'])): #φέρνω τα tokens ξανά μαζί διαχωριζόμενα με κενά\n",
        "    X['Article'][i] = ' '.join(X['Article'][i])\n",
        "print(X['Article'])\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0        vianet group inc announc unaudit second quarte...\n",
            "1        bionano genom report second quarter financi re...\n",
            "2        pyxi tanker announc date releas second quarter...\n",
            "3        intellig system announc new board member norcr...\n",
            "4        krato present canaccord virtual growth confer ...\n",
            "                               ...                        \n",
            "23862    grab slice pie resurg emerg market etf emerg m...\n",
            "23863    chipotl stock jump toward straight record wedb...\n",
            "23864    seattl base big fish game lay peopl read memo ...\n",
            "23865    molson coor steal stock valu illog take advant...\n",
            "23866    sylvania still look cheap palladium mine resta...\n",
            "Name: Article, Length: 23867, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v_A1300jfArI",
        "colab": {}
      },
      "source": [
        "X.to_csv (r'General_rnn.csv', index = False, header=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HmOB68OSfArK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "7d567f9e-5d7d-46d3-a319-ddb908a0e84f"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "batch=1024\n",
        "#TEXT = data.Field(tokenize='spacy',batch_first=True,include_lengths=True)\n",
        "TEXT = data.Field(batch_first=True,include_lengths=True)\n",
        "LABEL = data.LabelField(dtype = torch.long,batch_first=True)\n",
        "\n",
        "fields = [('text',TEXT),('label', LABEL)]\n",
        "Train_Data=data.TabularDataset(path = 'General_rnn.csv',format = 'csv',fields = fields,skip_header = True)\n",
        "\n",
        "X_train, X_test = Train_Data.split(split_ratio=0.7, random_state = random.seed(1234))\n",
        "X_train, X_val = X_train.split(split_ratio=0.8, random_state = random.seed(1234))\n",
        "\n",
        "TEXT.build_vocab(X_train, min_freq=2)  \n",
        "LABEL.build_vocab(X_train)\n",
        "\n",
        "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
        "\n",
        "#No. of unique tokens in label\n",
        "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
        "\n",
        "#Commonly used words\n",
        "print(TEXT.vocab.freqs.most_common(10))  \n",
        "train_it, val_it, test_it = data.BucketIterator.splits((X_train, X_val, X_test),sort_key = lambda x: len(x.text),\n",
        "    sort_within_batch=True,batch_size = batch,device = device)  \n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of TEXT vocabulary: 13043\n",
            "Size of LABEL vocabulary: 3\n",
            "[('stock', 5352), ('earn', 4718), ('inc', 4126), ('compani', 3759), ('announc', 3574), ('result', 3558), ('report', 2534), ('busi', 2310), ('new', 2168), ('investor', 2120)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hIvY8I8tfArM",
        "colab": {}
      },
      "source": [
        "def find_accuracy(preds, y):\n",
        "   \n",
        "    temp = torch.log_softmax(preds, dim = 1)\n",
        "    _, y_pred = torch.max(temp, dim = 1) \n",
        "    valid = (y_pred == y).float() \n",
        "    accur = valid.sum() / len(valid)\n",
        "    return accur\n",
        "\n",
        "def Loss_Optimizer (model,valueLR):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=valueLR)\n",
        "    return optimizer,criterion"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lNSC2cF4fArO",
        "colab": {}
      },
      "source": [
        "def train(model,data,lr,optimizer,criterion):\n",
        "        \n",
        "    #optimizer,criterion=Loss_Optimizer (model,lr)\n",
        "    #criterion = criterion.to(device)\n",
        "    model.train()\n",
        "    sumloss=0.0\n",
        "    sumacc=0.0\n",
        "    for i in data:\n",
        "        text, text_lengths = i.text   \n",
        "        optimizer.zero_grad()\n",
        "        pred = model(text, text_lengths).squeeze(0)       \n",
        "        loss = criterion(pred, i.label)\n",
        "        acc = find_accuracy(pred, i.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        sumloss += loss.item()\n",
        "        sumacc += acc.item()\n",
        "          \n",
        "    return sumloss / len(data), sumacc / len(data)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bqLTVN39fArR",
        "colab": {}
      },
      "source": [
        "def test(model, data, criterion):\n",
        "    \n",
        "    sumloss = 0\n",
        "    sumacc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i in data:\n",
        "            text, text_lengths = i.text\n",
        "            \n",
        "            pred = model(text, text_lengths).squeeze(0)\n",
        "           \n",
        "            loss = criterion(pred, i.label)\n",
        "            \n",
        "            acc = find_accuracy(pred, i.label)\n",
        "\n",
        "            sumloss += loss.item()\n",
        "            sumacc += acc.item()\n",
        "        \n",
        "    return sumloss / len(data), sumacc / len(data)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GnTkljWVfArT",
        "colab": {}
      },
      "source": [
        "def process_test(model,numEpochs,data_tr,data_val,data_test,lr,loss_file,acc_file,best_model):\n",
        "\n",
        "    optimizer,criterion=Loss_Optimizer (model,lr)\n",
        "    criterion = criterion.to(device)\n",
        "    start_time = time.time()\n",
        "    fileout=open(loss_file,\"w\")\n",
        "    fileout2=open(acc_file,\"w\")\n",
        "    \n",
        "    for i in range(numEpochs):\n",
        " \n",
        "        train_loss, train_acc = train(model, data_tr, lr,optimizer,criterion)\n",
        "        valid_loss, valid_acc = test(model, data_val, criterion)\n",
        "        fileout.write(str(valid_loss)+\"\\n\")\n",
        "        fileout2.write(str(valid_acc)+\"\\n\")\n",
        "        if i==0:\n",
        "            max=valid_acc\n",
        "            torch.save(model.state_dict(), best_model)\n",
        "        elif valid_acc > max:\n",
        "            max = valid_acc\n",
        "            torch.save(model.state_dict(), best_model)\n",
        "\n",
        "        if (i+1)%5==0:\n",
        "          print(\"Epoch : \",i+1,\" Train Loss : \",train_loss,\"  Train Acc : \",train_acc,\"  Valid Loss : \",valid_loss, \" Val Acc : \",valid_acc)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    timeHelp=(end_time-start_time)/60.0\n",
        "    print(\"\\nTime needed for Training : \",timeHelp)\n",
        "\n",
        "    fileout.close()\n",
        "    fileout2.close()  \n",
        "    \n",
        "   \n",
        "    model.load_state_dict(torch.load(best_model))\n",
        "    \n",
        "    test_loss, test_acc = test(model, data_test, criterion)\n",
        "    print(\"\\nLoss in Testset : \",test_loss,\"  Accuracy in Testset : \",test_acc,\"\\n\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OTVfTSDHfArW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82db3cb1-bf58-427d-9401-5a80c1fc2379"
      },
      "source": [
        "print(\"GPU Model   \",torch.cuda.get_device_name(0))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU Model    Tesla P4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iN7ov3gHfArY",
        "colab": {}
      },
      "source": [
        "class myLSTM(nn.Module):\n",
        "    def __init__(self, voc, embed_d, hid_d, out_d,dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(voc, embed_d)\n",
        "        self.rnn = nn.LSTM(embed_d,hid_d,batch_first=True,num_layers=2,bidirectional=True,dropout=dropout)\n",
        "        self.fc = nn.Linear(hid_d * 2, out_d)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, input_text, text_lengths):\n",
        "                \n",
        "        embedded = self.dropout(self.embedding(input_text))\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n",
        "        rnn_out,(rnn_hid,rnn_cell) = self.rnn(packed_embedded)\n",
        "        hidden = self.dropout(torch.cat((rnn_hid[-2,:,:], rnn_hid[-1,:,:]), dim=1))\n",
        "        return self.fc(hidden)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4OCyMqTxfArb",
        "colab": {}
      },
      "source": [
        "class myGRU(nn.Module):\n",
        "    def __init__(self, voc, embed_d, hid_d, out_d,dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(voc, embed_d)\n",
        "        self.rnn = nn.GRU(embed_d,hid_d,num_layers=2,bidirectional=True,dropout=dropout)\n",
        "        self.fc = nn.Linear(hid_d * 2, out_d)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, input_text,text_lengths):\n",
        "                \n",
        "        embedded = self.dropout(self.embedding(input_text))\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n",
        "        rnn_out,hidden = self.rnn(packed_embedded)\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "        return self.fc(hidden)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tMbHHUnifArd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "76907d50-ef15-4059-b631-5d776a714aca"
      },
      "source": [
        "myInput = len(TEXT.vocab)\n",
        "myEmbed = 300\n",
        "myHid = 256\n",
        "myOut = 3\n",
        "dropout = 0.5\n",
        "\n",
        "new_model2 = myLSTM(myInput, myEmbed, myHid, myOut,dropout)\n",
        "new_model2.to(torch.device(device))\n",
        "new_model3 = myGRU(myInput, myEmbed, myHid, myOut,dropout)\n",
        "new_model3.to(torch.device(device))\n",
        "\n",
        "print(new_model2)\n",
        "print(new_model3)\n",
        "#torch.save(new_model2.state_dict(), 'model_LSTM_1.pt')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "myLSTM(\n",
            "  (embedding): Embedding(13043, 300)\n",
            "  (rnn): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n",
            "myGRU(\n",
            "  (embedding): Embedding(13043, 300)\n",
            "  (rnn): GRU(300, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
            "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xEMSmzBOfArf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "ee0b369c-4428-4dbe-f29d-659348aba419"
      },
      "source": [
        "process_test(new_model2,30,train_it,val_it,test_it,0.001,\"lstm1_loss.txt\",\"lstm1_acc.txt\",\"best_LSTM_1_model.pt\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch :  5  Train Loss :  0.5368909218481609   Train Acc :  0.7792271205357143   Valid Loss :  0.45904945954680443  Val Acc :  0.8040331304073334\n",
            "Epoch :  10  Train Loss :  0.3272989824015115   Train Acc :  0.8643973214285714   Valid Loss :  0.4453469328582287  Val Acc :  0.8385123163461685\n",
            "Epoch :  15  Train Loss :  0.21343472559237853   Train Acc :  0.9167131696428571   Valid Loss :  0.6048370450735092  Val Acc :  0.8456867784261703\n",
            "Epoch :  20  Train Loss :  0.13702156721878314   Train Acc :  0.9461495535714286   Valid Loss :  0.6705305650830269  Val Acc :  0.8456867784261703\n",
            "Epoch :  25  Train Loss :  0.09885719674951231   Train Acc :  0.96240234375   Valid Loss :  0.7458628192543983  Val Acc :  0.8529638051986694\n",
            "Epoch :  30  Train Loss :  0.07908210468095993   Train Acc :  0.9715401785714286   Valid Loss :  0.7690333873033524  Val Acc :  0.8504280149936676\n",
            "\n",
            "Time needed for Training :  1.5869518240292868\n",
            "\n",
            "Loss in Testset :  0.9190350472927094   Accuracy in Testset :  0.8244274599211556 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ufEdn9rhfAri",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "323d14d8-1fa3-4003-9c71-ebad2b7dcd04"
      },
      "source": [
        "process_test(new_model3,30,train_it,val_it,test_it,0.001,\"gru1_loss.txt\",\"gru1_acc.txt\",\"best_gru_1_model.pt\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch :  5  Train Loss :  0.49289161073310034   Train Acc :  0.79296875   Valid Loss :  0.46295440196990967  Val Acc :  0.8070572018623352\n",
            "Epoch :  10  Train Loss :  0.33121166195321294   Train Acc :  0.8638392857142857   Valid Loss :  0.45033938623964787  Val Acc :  0.8353775143623352\n",
            "Epoch :  15  Train Loss :  0.24201753609980056   Train Acc :  0.9031110491071429   Valid Loss :  0.5072396621108055  Val Acc :  0.8452928960323334\n",
            "Epoch :  20  Train Loss :  0.17347335571581166   Train Acc :  0.9331752232142857   Valid Loss :  0.6225523538887501  Val Acc :  0.8498371690511703\n",
            "Epoch :  25  Train Loss :  0.1284193864516315   Train Acc :  0.94873046875   Valid Loss :  0.7187322080135345  Val Acc :  0.8504198491573334\n",
            "Epoch :  30  Train Loss :  0.10712693845354286   Train Acc :  0.9580775669642857   Valid Loss :  0.8060244657099247  Val Acc :  0.8415835946798325\n",
            "\n",
            "Time needed for Training :  1.410451642672221\n",
            "\n",
            "Loss in Testset :  0.6587474857057843   Accuracy in Testset :  0.8198192715644836 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}