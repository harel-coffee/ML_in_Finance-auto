{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "General_Rnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l9o-8aJZgk9I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "7252b3fa-75c8-492b-b0c3-0850c760cfb1"
      },
      "source": [
        ">>> import nltk\n",
        ">>> nltk.download('stopwords')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a5HnPotefAq7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8fff4e9-ade0-45d2-c861-9c537387e8fd"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torchtext import data\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import spacy\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"GPU Model   \",torch.cuda.get_device_name(0))\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU Model    Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VCJKt_MyfAq-",
        "colab": {}
      },
      "source": [
        "def clean_data(X):\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "    X = X.str.lower()\n",
        "    X = X.str.replace(\"[/(){}\\[\\]\\|@,;]\", \" \")\n",
        "    X = X.str.replace(\"[^0-9a-z #+_]\", \" \")\n",
        "    X = X.str.replace(r'\\d+','')\n",
        "    X = X.apply(lambda x: ' '.join([w for w in x.split() if (len(w)>2 and w not in STOPWORDS) ] ))\n",
        "    X = X.apply(lambda x: x.split()) \n",
        "    return X\n",
        "\n",
        "def target_arrange(y):\n",
        "    \n",
        "    for i in range(len(y)):\n",
        "        if y.values[i]==\"Negative\":\n",
        "            y.values[i]=0.0\n",
        "        elif y[i]==\"Positive\":\n",
        "            y.values[i]=1.0\n",
        "        else:\n",
        "            y.values[i]=2.0\n",
        "            \n",
        "    y=y.to_numpy()  \n",
        "    y=y.reshape(y.shape[0],1)\n",
        "    y= pd.DataFrame(data=y)\n",
        "    y=np.ravel(y)\n",
        "    y=y.astype('float')\n",
        "    return y"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "39r_65tmfArB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "0957e93c-d677-4107-94fb-619200b3e7c0"
      },
      "source": [
        "with open(\"All_Tickers.json\",\"r\") as fp:\n",
        "#with open(\"General_Market.json\",encoding='utf8') as fp:\n",
        "    json_d = json.load(fp)\n",
        " \n",
        "ticks_d = json_d['data']\n",
        "df = pd.DataFrame(ticks_d)\n",
        "\n",
        "X= pd.DataFrame(columns=['Date', 'Article','Target'])\n",
        "X['Date']=pd.to_datetime(df['date'])\n",
        "X['Article']=df['title']+\" \"+df['text']\n",
        "X['Target']=df['sentiment']\n",
        "\n",
        "X=X.sort_values(\"Date\")\n",
        "\n",
        "print(\"Number of Examples : \",len(X),\"\\n\")\n",
        "X.drop_duplicates(inplace=True)\n",
        "X.index = range(len(X))\n",
        "print(\"Number of Examples after removing duplicates: \",len(X),\"\\n\")\n",
        "\n",
        "print('Number of words before cleaning : ',X['Article'].apply(lambda x: len(str(x).split(' '))).sum())\n",
        "X['Article']=clean_data(X['Article'])\n",
        "print('Number of words after cleaning : ',X['Article'].apply(lambda x: len(str(x).split(' '))).sum())\n",
        "print(\"\\n******************\\n\")\n",
        "\n",
        "X['Target']=target_arrange(X['Target'])\n",
        "X=X.drop('Date',1)\n",
        "L=[]\n",
        "L.append((X['Target']==0.0).sum())\n",
        "L.append((X['Target']==1.0).sum())\n",
        "L.append((X['Target']==2.0).sum())\n",
        "\n",
        "print(\"Negative Examples : \",(X['Target']==0.0).sum())\n",
        "print(\"Positive Examples : \",(X['Target']==1.0).sum())\n",
        "print(\"Neutral Examples : \",(X['Target']==2.0).sum())\n",
        "\n",
        "maximum=max(L)\n",
        "\n",
        "Weights=[]\n",
        "\n",
        "for i in L:\n",
        "  Weights.append(maximum/i)\n",
        "  \n",
        "class_weights = torch.FloatTensor(Weights).to(device)\n",
        "print(\"\\n Weights = \",class_weights)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Examples :  25630 \n",
            "\n",
            "Number of Examples after removing duplicates:  25464 \n",
            "\n",
            "Number of words before cleaning :  973488\n",
            "Number of words after cleaning :  655286\n",
            "\n",
            "******************\n",
            "\n",
            "Negative Examples :  2819\n",
            "Positive Examples :  9006\n",
            "Neutral Examples :  13639\n",
            "\n",
            " Weights =  tensor([4.8382, 1.5144, 1.0000], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F2Lyyf7vfArF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "3ad92c47-9178-4eaa-d319-a2b4d088ec55"
      },
      "source": [
        "X['Article']= X['Article'].apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
        "for i in range(len(X['Article'])): #φέρνω τα tokens ξανά μαζί διαχωριζόμενα με κενά\n",
        "    X['Article'][i] = ' '.join(X['Article'][i])\n",
        "print(X['Article'])\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0        vianet group inc announc unaudit second quarte...\n",
            "1        krato present canaccord virtual growth confer ...\n",
            "2        rewalk robot report second quarter financi res...\n",
            "3        pyxi tanker announc date releas second quarter...\n",
            "4        bionano genom report second quarter financi re...\n",
            "                               ...                        \n",
            "25459    rocket compani earn rkt stock move releas resu...\n",
            "25460    histor day truliev introduc edibl florida serv...\n",
            "25461    shopifi stock declin today investor cash recen...\n",
            "25462    bluelinx announc registr statement updat marie...\n",
            "25463    peloton stock jump record high today analyst s...\n",
            "Name: Article, Length: 25464, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v_A1300jfArI",
        "colab": {}
      },
      "source": [
        "X.to_csv (r'General_rnn.csv', index = False, header=True)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HmOB68OSfArK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "1e4d244b-ecd0-4b8f-a5c1-8a1007a5dc87"
      },
      "source": [
        "torch.manual_seed(1234)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "batch=1024\n",
        "#TEXT = data.Field(tokenize='spacy',batch_first=True,include_lengths=True)\n",
        "TEXT = data.Field(batch_first=True,include_lengths=True)\n",
        "LABEL = data.LabelField(dtype = torch.long,batch_first=True)\n",
        "\n",
        "fields = [('text',TEXT),('label', LABEL)]\n",
        "Train_Data=data.TabularDataset(path = 'General_rnn.csv',format = 'csv',fields = fields,skip_header = True)\n",
        "\n",
        "X_train, X_test = Train_Data.split(split_ratio=0.7, random_state = random.seed(1234))\n",
        "X_train, X_val = X_train.split(split_ratio=0.8, random_state = random.seed(1234))\n",
        "\n",
        "TEXT.build_vocab(X_train, min_freq=2)  \n",
        "LABEL.build_vocab(X_train)\n",
        "\n",
        "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
        "\n",
        "#No. of unique tokens in label\n",
        "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
        "\n",
        "#Commonly used words\n",
        "print(TEXT.vocab.freqs.most_common(10))  \n",
        "train_it, val_it, test_it = data.BucketIterator.splits((X_train, X_val, X_test),sort_key = lambda x: len(x.text),\n",
        "    sort_within_batch=True,batch_size = batch,device = device)  \n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of TEXT vocabulary: 13543\n",
            "Size of LABEL vocabulary: 3\n",
            "[('stock', 5729), ('earn', 4935), ('inc', 4416), ('compani', 3996), ('announc', 3875), ('result', 3655), ('report', 2661), ('busi', 2552), ('new', 2375), ('investor', 2277)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hIvY8I8tfArM",
        "colab": {}
      },
      "source": [
        "def find_accuracy(preds, y):\n",
        "   \n",
        "    temp = torch.log_softmax(preds, dim = 1)\n",
        "    _, y_pred = torch.max(temp, dim = 1) \n",
        "    valid = (y_pred == y).float() \n",
        "    accur = valid.sum() / len(valid)\n",
        "    return accur\n",
        "\n",
        "def Loss_Optimizer (model,valueLR):\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=valueLR)\n",
        "    return optimizer,criterion"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lNSC2cF4fArO",
        "colab": {}
      },
      "source": [
        "def train(model,data,lr,optimizer,criterion):\n",
        "        \n",
        "    #optimizer,criterion=Loss_Optimizer (model,lr)\n",
        "    #criterion = criterion.to(device)\n",
        "    model.train()\n",
        "    sumloss=0.0\n",
        "    sumacc=0.0\n",
        "    for i in data:\n",
        "        text, text_lengths = i.text   \n",
        "        optimizer.zero_grad()\n",
        "        pred = model(text, text_lengths).squeeze(0)       \n",
        "        loss = criterion(pred, i.label)\n",
        "        acc = find_accuracy(pred, i.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        sumloss += loss.item()\n",
        "        sumacc += acc.item()\n",
        "          \n",
        "    return sumloss / len(data), sumacc / len(data)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bqLTVN39fArR",
        "colab": {}
      },
      "source": [
        "def test(model, data, criterion):\n",
        "    \n",
        "    sumloss = 0\n",
        "    sumacc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i in data:\n",
        "            text, text_lengths = i.text\n",
        "            \n",
        "            pred = model(text, text_lengths).squeeze(0)\n",
        "           \n",
        "            loss = criterion(pred, i.label)\n",
        "            \n",
        "            acc = find_accuracy(pred, i.label)\n",
        "\n",
        "            sumloss += loss.item()\n",
        "            sumacc += acc.item()\n",
        "        \n",
        "    return sumloss / len(data), sumacc / len(data)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GnTkljWVfArT",
        "colab": {}
      },
      "source": [
        "def process_test(model,numEpochs,data_tr,data_val,data_test,lr,loss_file,acc_file,best_model):\n",
        "\n",
        "    optimizer,criterion=Loss_Optimizer (model,lr)\n",
        "    criterion = criterion.to(device)\n",
        "    start_time = time.time()\n",
        "    fileout=open(loss_file,\"w\")\n",
        "    fileout2=open(acc_file,\"w\")\n",
        "    \n",
        "    for i in range(numEpochs):\n",
        " \n",
        "        train_loss, train_acc = train(model, data_tr, lr,optimizer,criterion)\n",
        "        valid_loss, valid_acc = test(model, data_val, criterion)\n",
        "        fileout.write(str(valid_loss)+\"\\n\")\n",
        "        fileout2.write(str(valid_acc)+\"\\n\")\n",
        "        if i==0:\n",
        "            max=valid_acc\n",
        "            torch.save(model.state_dict(), best_model)\n",
        "        elif valid_acc > max:\n",
        "            max = valid_acc\n",
        "            torch.save(model.state_dict(), best_model)\n",
        "\n",
        "        if (i+1)%5==0:\n",
        "          print(\"Epoch : \",i+1,\" Train Loss : \",train_loss,\"  Train Acc : \",train_acc,\"  Valid Loss : \",valid_loss, \" Val Acc : \",valid_acc)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    timeHelp=(end_time-start_time)/60.0\n",
        "    print(\"\\nTime needed for Training : \",timeHelp)\n",
        "\n",
        "    fileout.close()\n",
        "    fileout2.close()  \n",
        "    \n",
        "   \n",
        "    model.load_state_dict(torch.load(best_model))\n",
        "    \n",
        "    test_loss, test_acc = test(model, data_test, criterion)\n",
        "    print(\"\\nLoss in Testset : \",test_loss,\"  Accuracy in Testset : \",test_acc,\"\\n\")"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OTVfTSDHfArW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dab1537d-c7b8-4c4d-a544-1aaf17744651"
      },
      "source": [
        "print(\"GPU Model   \",torch.cuda.get_device_name(0))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU Model    Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iN7ov3gHfArY",
        "colab": {}
      },
      "source": [
        "class myLSTM(nn.Module):\n",
        "    def __init__(self, voc, embed_d, hid_d, out_d,dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(voc, embed_d)\n",
        "        self.rnn = nn.LSTM(embed_d,hid_d,batch_first=True,num_layers=2,bidirectional=True,dropout=dropout)\n",
        "        self.fc = nn.Linear(hid_d * 2, out_d)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, input_text, text_lengths):\n",
        "                \n",
        "        embedded = self.dropout(self.embedding(input_text))\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n",
        "        rnn_out,(rnn_hid,rnn_cell) = self.rnn(packed_embedded)\n",
        "        hidden = self.dropout(torch.cat((rnn_hid[-2,:,:], rnn_hid[-1,:,:]), dim=1))\n",
        "        return self.fc(hidden)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4OCyMqTxfArb",
        "colab": {}
      },
      "source": [
        "class myGRU(nn.Module):\n",
        "    def __init__(self, voc, embed_d, hid_d, out_d,dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(voc, embed_d)\n",
        "        self.rnn = nn.GRU(embed_d,hid_d,num_layers=2,bidirectional=True,dropout=dropout)\n",
        "        self.fc = nn.Linear(hid_d * 2, out_d)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, input_text,text_lengths):\n",
        "                \n",
        "        embedded = self.dropout(self.embedding(input_text))\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n",
        "        rnn_out,hidden = self.rnn(packed_embedded)\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "        return self.fc(hidden)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tMbHHUnifArd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "be84e45b-5aa0-4f7b-ead0-1bc86bfa825a"
      },
      "source": [
        "myInput = len(TEXT.vocab)\n",
        "myEmbed = 300\n",
        "myHid = 256\n",
        "myOut = 3\n",
        "dropout = 0.4\n",
        "\n",
        "new_model2 = myLSTM(myInput, myEmbed, myHid, myOut,dropout)\n",
        "new_model2.to(torch.device(device))\n",
        "new_model3 = myGRU(myInput, myEmbed, myHid, myOut,dropout)\n",
        "new_model3.to(torch.device(device))\n",
        "\n",
        "print(new_model2)\n",
        "print(new_model3)\n",
        "#torch.save(new_model2.state_dict(), 'model_LSTM_1.pt')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "myLSTM(\n",
            "  (embedding): Embedding(13543, 300)\n",
            "  (rnn): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
            "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
            "  (dropout): Dropout(p=0.4, inplace=False)\n",
            ")\n",
            "myGRU(\n",
            "  (embedding): Embedding(13543, 300)\n",
            "  (rnn): GRU(300, 256, num_layers=2, dropout=0.4, bidirectional=True)\n",
            "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
            "  (dropout): Dropout(p=0.4, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xEMSmzBOfArf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "dbec7a23-d715-45e6-d582-50b7335e2e70"
      },
      "source": [
        "process_test(new_model2,30,train_it,val_it,test_it,0.001,\"lstm1_loss.txt\",\"lstm1_acc.txt\",\"best_LSTM_1_model.pt\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch :  5  Train Loss :  0.3213204635041101   Train Acc :  0.7504517819200244   Valid Loss :  0.33841373212635517  Val Acc :  0.7604212760925293\n",
            "Epoch :  10  Train Loss :  0.18096093354480608   Train Acc :  0.8729685800416129   Valid Loss :  0.3649232964962721  Val Acc :  0.8099070489406586\n",
            "Epoch :  15  Train Loss :  0.09438559313171677   Train Acc :  0.9359904144491468   Valid Loss :  0.5414566807448864  Val Acc :  0.8338892906904221\n",
            "Epoch :  20  Train Loss :  0.05695255466603807   Train Acc :  0.9607532492705754   Valid Loss :  0.6892155893146992  Val Acc :  0.8343963921070099\n",
            "Epoch :  25  Train Loss :  0.04048797596312527   Train Acc :  0.9723604534353528   Valid Loss :  0.805655162781477  Val Acc :  0.8415140956640244\n",
            "Epoch :  30  Train Loss :  0.029339092584060773   Train Acc :  0.9816322028636932   Valid Loss :  0.8945030122995377  Val Acc :  0.8291565328836441\n",
            "\n",
            "Time needed for Training :  1.4800446232159932\n",
            "\n",
            "Loss in Testset :  0.8138916416792199   Accuracy in Testset :  0.8371740207076073 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ufEdn9rhfAri",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "b03e472f-9d05-42d6-a222-b4fd73ce26e5"
      },
      "source": [
        "process_test(new_model3,30,train_it,val_it,test_it,0.001,\"gru1_loss.txt\",\"gru1_acc.txt\",\"best_gru_1_model.pt\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch :  5  Train Loss :  0.32312021191631046   Train Acc :  0.7521788690771375   Valid Loss :  0.34765241108834743  Val Acc :  0.7750697135925293\n",
            "Epoch :  10  Train Loss :  0.2063138157661472   Train Acc :  0.8567967287131718   Valid Loss :  0.4097174350172281  Val Acc :  0.8103577047586441\n",
            "Epoch :  15  Train Loss :  0.1286780090470399   Train Acc :  0.9168776912348611   Valid Loss :  0.47487685456871986  Val Acc :  0.8295512050390244\n",
            "Epoch :  20  Train Loss :  0.09132574531914932   Train Acc :  0.9373855037348611   Valid Loss :  0.6166034489870071  Val Acc :  0.8290629237890244\n",
            "Epoch :  25  Train Loss :  0.05721193834740136   Train Acc :  0.9627984975065503   Valid Loss :  0.6945605911314487  Val Acc :  0.8246683925390244\n",
            "Epoch :  30  Train Loss :  0.039635455468669534   Train Acc :  0.9736857882567814   Valid Loss :  0.8064631782472134  Val Acc :  0.8319738060235977\n",
            "\n",
            "Time needed for Training :  1.250110912322998\n",
            "\n",
            "Loss in Testset :  0.6560244250576943   Accuracy in Testset :  0.8362770229578018 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}