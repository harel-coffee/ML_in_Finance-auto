{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection,metrics,naive_bayes,preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "stemmer = PorterStemmer()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(X):\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    X=X.str.lower()\n",
    "    X=X.str.replace(\"[/(){}\\[\\]\\|@,;]\", \" \")\n",
    "    X=X.str.replace(\"[^0-9a-z #+_]\", \" \")\n",
    "    X = X.str.replace(r'\\d+','')\n",
    "    X = X.apply(lambda x: ' '.join([w for w in x.split() if (len(w)>2 and w not in STOPWORDS) ] ))\n",
    "    X = X.apply(lambda x: x.split()) \n",
    "    return X\n",
    "\n",
    "def target_arrange(y):\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        if y.values[i]==\"Negative\":\n",
    "            y.values[i]=0.0\n",
    "        elif y[i]==\"Positive\":\n",
    "            y.values[i]=1.0\n",
    "        else:\n",
    "            y.values[i]=2.0\n",
    "            \n",
    "    y=y.to_numpy()  \n",
    "    y=y.reshape(y.shape[0],1)\n",
    "    y= pd.DataFrame(data=y)\n",
    "    y=np.ravel(y)\n",
    "    y=y.astype('float')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Examples :  24030 \n",
      "\n",
      "Number of Examples after removing duplicates:  23867 \n",
      "\n",
      "Number of words before cleaning :  910682\n",
      "Number of words after cleaning :  612873\n",
      "           Date  Article\n",
      "Target                  \n",
      "Negative   2640     2640\n",
      "Neutral   12762    12762\n",
      "Positive   8465     8465 \n",
      "\n",
      "0       2020-08-07 08:00:00-04:00\n",
      "1       2020-08-07 08:00:00-04:00\n",
      "2       2020-08-07 08:00:00-04:00\n",
      "3       2020-08-07 08:00:00-04:00\n",
      "4       2020-08-07 08:00:00-04:00\n",
      "                   ...           \n",
      "23862   2020-09-01 14:51:53-04:00\n",
      "23863   2020-09-01 14:52:59-04:00\n",
      "23864   2020-09-01 14:54:43-04:00\n",
      "23865   2020-09-01 14:58:39-04:00\n",
      "23866   2020-09-01 14:59:18-04:00\n",
      "Name: Date, Length: 23867, dtype: datetime64[ns, pytz.FixedOffset(-240)]\n"
     ]
    }
   ],
   "source": [
    "with open(\"All_Tickers.json\",\"r\") as fp:\n",
    "#with open(\"General_Market.json\",encoding='utf8') as fp:\n",
    "    json_d = json.load(fp)\n",
    " \n",
    "ticks_d = json_d['data']\n",
    "df = pd.DataFrame(ticks_d)\n",
    "\n",
    "X= pd.DataFrame(columns=['Date', 'Article','Target'])\n",
    "X['Date']=pd.to_datetime(df['date'])\n",
    "X['Article']=df['title']+\" \"+df['text']\n",
    "X['Target']=df['sentiment']\n",
    "\n",
    "X=X.sort_values(\"Date\")\n",
    "\n",
    "print(\"Number of Examples : \",len(X),\"\\n\")\n",
    "X.drop_duplicates(inplace=True)\n",
    "X.index = range(len(X))\n",
    "print(\"Number of Examples after removing duplicates: \",len(X),\"\\n\")\n",
    "\n",
    "\n",
    "X.to_csv (r'General.csv', index = False, header=True)\n",
    "\n",
    "print('Number of words before cleaning : ',X['Article'].apply(lambda x: len(str(x).split(' '))).sum())\n",
    "X['Article']=clean_data(X['Article'])\n",
    "print('Number of words after cleaning : ',X['Article'].apply(lambda x: len(str(x).split(' '))).sum())\n",
    "\n",
    "print(X.groupby(['Target']).count(),\"\\n\")\n",
    "print(X['Date'])\n",
    "y=target_arrange(X['Target'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        vianet group inc announc unaudit second quarte...\n",
      "1        bionano genom report second quarter financi re...\n",
      "2        pyxi tanker announc date releas second quarter...\n",
      "3        intellig system announc new board member norcr...\n",
      "4        krato present canaccord virtual growth confer ...\n",
      "                               ...                        \n",
      "23862    grab slice pie resurg emerg market etf emerg m...\n",
      "23863    chipotl stock jump toward straight record wedb...\n",
      "23864    seattl base big fish game lay peopl read memo ...\n",
      "23865    molson coor steal stock valu illog take advant...\n",
      "23866    sylvania still look cheap palladium mine resta...\n",
      "Name: Article, Length: 23867, dtype: object\n"
     ]
    }
   ],
   "source": [
    "X['Article']= X['Article'].apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "for i in range(len(X['Article'])): #φέρνω τα tokens ξανά μαζί διαχωριζόμενα με κενά\n",
    "    X['Article'][i] = ' '.join(X['Article'][i])\n",
    "\n",
    "print(X['Article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9)\n",
    "Xv = tfidf_vectorizer.fit_transform(X['Article'])\n",
    "Xv = pd.DataFrame(Xv.todense())\n",
    "X_train,X_test,y_train,y_test = train_test_split(Xv,y, test_size=0.3,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Recall metric: 0.6803532643151754\n",
      " F1 metric: 0.6921713760677429\n",
      " Precision metric: 0.7291425271175639\n",
      " Accuracy metric: 0.7644183773216031\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1 # This is the smoothing parameter for Laplace/Lidstone smoothing\n",
    "model = naive_bayes.MultinomialNB(alpha=alpha)\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "\n",
    "recall = metrics.recall_score(y_test,y_predicted,average='macro')\n",
    "precision = metrics.precision_score(y_test,y_predicted,average='macro')\n",
    "f1 = metrics.f1_score(y_test,y_predicted,average='macro')\n",
    "Accur=metrics.accuracy_score(y_test,y_predicted)\n",
    "\n",
    "print(' Recall metric:',recall)\n",
    "print(' F1 metric:',f1)\n",
    "print(' Precision metric:',precision)\n",
    "print(' Accuracy metric:',Accur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7991770143308028\n"
     ]
    }
   ],
   "source": [
    "skfold = model_selection.StratifiedKFold(n_splits=5)\n",
    "model2 = RandomForestClassifier(n_estimators=100, max_features=\"auto\", n_jobs=-1)\n",
    "results = model_selection.cross_val_score(model2, Xv,y, cv=skfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Recall metric: 0.7011104090909471\n",
      " F1 metric: 0.7218918934432083\n",
      " Precision metric: 0.8110436742344805\n",
      " Accuracy metric: 0.807149839407904\n"
     ]
    }
   ],
   "source": [
    "model2 = RandomForestClassifier(n_estimators=100, max_features=\"auto\", n_jobs=-1)\n",
    "model2.fit(X_train,y_train)\n",
    "y_predicted = model2.predict(X_test)\n",
    "\n",
    "\n",
    "recall = metrics.recall_score(y_test,y_predicted,average='macro')\n",
    "precision = metrics.precision_score(y_test,y_predicted,average='macro')\n",
    "f1 = metrics.f1_score(y_test,y_predicted,average='macro')\n",
    "Accur=metrics.accuracy_score(y_test,y_predicted)\n",
    "\n",
    "print(' Recall metric:',recall)\n",
    "print(' F1 metric:',f1)\n",
    "print(' Precision metric:',precision)\n",
    "print(' Accuracy metric:',Accur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
