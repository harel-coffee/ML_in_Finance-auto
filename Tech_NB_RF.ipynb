{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection,metrics,naive_bayes,preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "stemmer = PorterStemmer()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(X):\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    X=X.str.lower()\n",
    "    X=X.str.replace(\"[/(){}\\[\\]\\|@,;]\", \" \")\n",
    "    X=X.str.replace(\"[^0-9a-z #+_]\", \" \")\n",
    "    X = X.str.replace(r'\\d+','')\n",
    "    X = X.apply(lambda x: ' '.join([w for w in str(x).split() if (len(w)>2 and w not in STOPWORDS) ] ))\n",
    "    X = X.apply(lambda x: x.split()) \n",
    "    return X\n",
    "\n",
    "def target_arrange(y):\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        if y.values[i]==\"Negative\":\n",
    "            y.values[i]=0.0\n",
    "        elif y[i]==\"Positive\":\n",
    "            y.values[i]=1.0\n",
    "        else:\n",
    "            y.values[i]=2.0\n",
    "            \n",
    "    y=y.to_numpy()  \n",
    "    y=y.reshape(y.shape[0],1)\n",
    "    y= pd.DataFrame(data=y)\n",
    "    y=np.ravel(y)\n",
    "    y=y.astype('float')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Examples :  15641 \n",
      "\n",
      "Number of Examples after removing duplicates:  15432 \n",
      "\n",
      "Number of words before cleaning :  585553\n",
      "Number of words after cleaning :  393507\n",
      "          Date  Article\n",
      "Target                 \n",
      "Negative  1892     1892\n",
      "Neutral   7702     7702\n",
      "Positive  5838     5838\n"
     ]
    }
   ],
   "source": [
    "with open(\"Tech_news.json\",\"r\") as fp:\n",
    "#with open(\"General_Market.json\",encoding='utf8') as fp:\n",
    "    json_d = json.load(fp)\n",
    " \n",
    "ticks_d = json_d['data']\n",
    "df = pd.DataFrame(ticks_d)\n",
    "\n",
    "X= pd.DataFrame(columns=['Date', 'Article','Target'])\n",
    "X['Date']=pd.to_datetime(df['date'])\n",
    "X['Article']=df['title']+\" \"+df['text']\n",
    "X['Target']=df['sentiment']\n",
    "\n",
    "X=X.sort_values(\"Date\")\n",
    "print(\"Number of Examples : \",len(X),\"\\n\")\n",
    "X.drop_duplicates(inplace=True)\n",
    "X.index = range(len(X))\n",
    "print(\"Number of Examples after removing duplicates: \",len(X),\"\\n\")\n",
    "X.to_csv (r'Tech.csv', index = False, header=True)\n",
    "\n",
    "print('Number of words before cleaning : ',X['Article'].apply(lambda x: len(str(x).split(' '))).sum())\n",
    "X['Article']=clean_data(X['Article'])\n",
    "print('Number of words after cleaning : ',X['Article'].apply(lambda x: len(str(x).split(' '))).sum())\n",
    "\n",
    "print(X.groupby(['Target']).count())\n",
    "\n",
    "y=target_arrange(X['Target'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       2020-06-25 16:15:00-04:00\n",
      "1       2020-06-25 16:15:00-04:00\n",
      "2       2020-06-25 16:15:00-04:00\n",
      "3       2020-06-25 16:18:00-04:00\n",
      "4       2020-06-25 16:21:36-04:00\n",
      "                   ...           \n",
      "15427   2020-09-01 14:23:00-04:00\n",
      "15428   2020-09-01 14:27:40-04:00\n",
      "15429   2020-09-01 14:35:33-04:00\n",
      "15430   2020-09-01 14:36:34-04:00\n",
      "15431   2020-09-01 14:40:38-04:00\n",
      "Name: Date, Length: 15432, dtype: datetime64[ns, pytz.FixedOffset(-240)]    0        mercuri system receiv contract award base new ...\n",
      "1        progress second quarter revenu exce guidanc in...\n",
      "2        acuiti brand declar quarterli dividend atlanta...\n",
      "3         share factset soar today earn came better expect\n",
      "4        stifel say inseego leader inseego corp nasdaq ...\n",
      "                               ...                        \n",
      "15427    lead plaintiff deadlin alert faruqi faruqi llp...\n",
      "15428    taiwan semiconductor stock best chip make game...\n",
      "15429    twilio stock rock star may done yet twilio sto...\n",
      "15430    incred stat zoom follow blowout earn report zo...\n",
      "15431    explor sale xandr advertis unit amid restructu...\n",
      "Name: Article, Length: 15432, dtype: object\n"
     ]
    }
   ],
   "source": [
    "X['Article']= X['Article'].apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "for i in range(len(X['Article'])): #φέρνω τα tokens ξανά μαζί διαχωριζόμενα με κενά\n",
    "    X['Article'][i] = ' '.join(X['Article'][i])\n",
    "\n",
    "print(X['Date'],\"  \",X['Article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9)\n",
    "Xv = tfidf_vectorizer.fit_transform(X['Article'])\n",
    "Xv = pd.DataFrame(Xv.todense())\n",
    "X_train,X_test,y_train,y_test = train_test_split(Xv,y, test_size=0.3,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Recall metric: 0.6949830569513687\n",
      " F1 metric: 0.7023892187545657\n",
      " Precision metric: 0.7208625956083868\n",
      " Accuracy metric: 0.7587473002159827\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1 # This is the smoothing parameter for Laplace/Lidstone smoothing\n",
    "model = naive_bayes.MultinomialNB(alpha=alpha)\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "\n",
    "recall = metrics.recall_score(y_test,y_predicted,average='macro')\n",
    "precision = metrics.precision_score(y_test,y_predicted,average='macro')\n",
    "f1 = metrics.f1_score(y_test,y_predicted,average='macro')\n",
    "Accur=metrics.accuracy_score(y_test,y_predicted)\n",
    "\n",
    "print(' Recall metric:',recall)\n",
    "print(' F1 metric:',f1)\n",
    "print(' Precision metric:',precision)\n",
    "print(' Accuracy metric:',Accur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7857074416348028\n"
     ]
    }
   ],
   "source": [
    "skfold = model_selection.StratifiedKFold(n_splits=5)\n",
    "model2 = RandomForestClassifier(n_estimators=100, max_features=\"auto\", n_jobs=-1)\n",
    "results = model_selection.cross_val_score(model2, Xv,y, cv=skfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Recall metric: 0.7137764603299449\n",
      " F1 metric: 0.737260315385479\n",
      " Precision metric: 0.8085507532283159\n",
      " Accuracy metric: 0.8010799136069114\n"
     ]
    }
   ],
   "source": [
    "model2 = RandomForestClassifier(n_estimators=100, max_features=\"auto\", n_jobs=-1)\n",
    "model2.fit(X_train,y_train)\n",
    "y_predicted = model2.predict(X_test)\n",
    "\n",
    "\n",
    "recall = metrics.recall_score(y_test,y_predicted,average='macro')\n",
    "precision = metrics.precision_score(y_test,y_predicted,average='macro')\n",
    "f1 = metrics.f1_score(y_test,y_predicted,average='macro')\n",
    "Accur=metrics.accuracy_score(y_test,y_predicted)\n",
    "\n",
    "print(' Recall metric:',recall)\n",
    "print(' F1 metric:',f1)\n",
    "print(' Precision metric:',precision)\n",
    "print(' Accuracy metric:',Accur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
